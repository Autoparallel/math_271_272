%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document data
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt]{article} %report allows for chapters
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{preamble}

\begin{document}

\begin{center}
   \textsc{\large MATH 271, Homework 8, \emph{Solutions}}\\
\end{center}
\vspace{.5cm}

\begin{problem}
Let a mass $m_1$ weighing $1kg.$ be placed at $\vecr_1=2\xhat -3 \yhat -\zhat$ and a mass $m_2$ of $2kg.$ be placed at $\vecr_2 = 4\yhat -2\zhat$.  Where must a mass $m_3$ of $3kg.$ be placed so that the center of mass is at the origin $\zerovec$?
\end{problem}
\begin{solution}
One can compute the center of mass $\boldsymbol{\vec{R}}_{CM}$ by
\[
\boldsymbol{\vec{R}}_{CM} = \frac{m_1\vecr_1 + m_2\vecr_2 + m_3 \vecr_3}{m_1 + m_2 + m_3}.
\]
Here, we know everything but $\vecr_3$.  Since we want the center of mass at the origin $\zerovec$, then
\begin{align*}
\zerovec &= \frac{1}{m_1+m_2 + m_3} \left( m_1 \vecr_1 + m_2 \vecr_2 + m_3 \vecr_3 \right)\\
\begin{pmatrix} 0 \\ 0 \\ 0\end{pmatrix}&= \frac{1}{6} \left( \begin{pmatrix} 2 \\ -3 \\ -1 \end{pmatrix} + 2 \begin{pmatrix} 0 \\ 4 \\ -2 \end{pmatrix} + 3 \begin{pmatrix} x \\ y \\ z \end{pmatrix}\right).
\end{align*}
What we have above is three equations and three unknowns. That is, one equation for the $\xhat$-component, one for the $\yhat$-component, and one for the $\zhat$-component. We have
\begin{align*}
    0 & = \frac{1}{6} (2 + 2\cdot 0 + 3x)\\
    0 & = \frac{1}{6} (-3 + 2\cdot 4 + 3y)\\
    0 & = \frac{1}{6} (-1 +2\cdot (-2) + 3z).
\end{align*}
Taking the first, we find
\begin{align*}
    0&= \frac{1}{3} +\frac{1}{2}x\\
    -\frac{1}{3}&= \frac{1}{2}x\\
    \implies~ x&= -\frac{2}{3}.
\end{align*}
Next,
\begin{align*}
    0&= -\frac{1}{2} +\frac{4}{3}+\frac{1}{2}y\\
    -\frac{5}{6}&= \frac{1}{2}y\\
    \implies~ y&= -\frac{5}{3}.
\end{align*}
Lastly, we have
\begin{align*}
    0&= -\frac{1}{6}-\frac{2}{3} +\frac{1}{2}z\\
    \frac{5}{6} &= \frac{1}{2}z\\
    \implies~ z&= \frac{5}{3}.
\end{align*}
Thus we have that $\vecr_3 = -\frac{2}{3} \xhat - \frac{5}{3}\yhat + \frac{5}{3}\zhat.$
\end{solution}

\newpage
\begin{problem}
Which of the following are linear transformations? For those that are not, which properties of \emph{linearity} (the properties (i) and (ii) in our notes) fail? Show your work.
\begin{enumerate}[(a)]
    \item $T_a \colon \R \to \R$ given by $T_a(x)=\frac{1}{x}$.
    \item $T_b \colon \R^3 \to \R^2$ given by
    \[
    T_b \begin{pmatrix} x\\ y\\ z \end{pmatrix}
    = \begin{pmatrix} x\\ y \end{pmatrix}.
    \]
    \item $T_c \colon \R \to \R^3$ given by
    \[
    T_c(t)=\begin{pmatrix} t\\ t^2\\ t^3 \end{pmatrix}.
    \]
    \item $T_d \colon \R^2 \to \R^3$ given by
    \[
    T_d \begin{pmatrix} x\\ y \end{pmatrix}
    = \begin{pmatrix} x+y\\ x+y\\ x+y \end{pmatrix}.
    \]
\end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item This transformation fails both properties.  For (i), take
    \[
    T_a(x+y) = \frac{1}{x+y} \neq \frac{1}{x}+\frac{1}{y} = T_a(x)+T_a(y).
    \]
    For (ii), take
    \[
    T_a(\alpha x) = \frac{1}{\alpha x} \neq \alpha \frac{1}{x} = \alpha T_a(x).
    \]
    \item This is a linear transformation.  To see (i) holds, take
    \begin{align*}
        T_b(\vecu +\vecv)&= T_b \left( \begin{pmatrix} u_x \\ u_y \\ u_z \end{pmatrix} + \begin{pmatrix} v_x \\ v_y \\ v_z \end{pmatrix} \right)\\
        &=T_b \begin{pmatrix} u_x + v_x \\ u_y + v_y \\ u_z + v_z \end{pmatrix}\\
        &= \begin{pmatrix} u_x + v_x \\ u_y + v_y \end{pmatrix}\\
        &= \begin{pmatrix} u_x \\ u_y \end{pmatrix} + \begin{pmatrix} v_x \\ v_y \end{pmatrix}\\
        &= T_b(\vecu) + T_b(\vecv).
    \end{align*}
    And for (ii), we take
    \begin{align*}
        T_b(\alpha \vecv) &= T_b \left( \alpha \begin{pmatrix} v_x \\ v_y \\ v_z \end{pmatrix}\right)\\
        &= T_b \begin{pmatrix} \alpha v_x \\ \alpha v_y \\ \alpha v_z \end{pmatrix}\\
        &= \begin{pmatrix} \alpha v_x \\ \alpha v_y \end{pmatrix}\\
        &= \alpha \begin{pmatrix} v_x \\ v_y \end{pmatrix}\\
        &= \alpha T_b(\vecv).
    \end{align*}
    \item This is not a linear transformation as both properties fail. Indeed, for (i) we take
    \begin{align*}
        T_c(\vecu +\vecv) &= T_c \left( \begin{pmatrix} u_x \\ u_y \\ u_z \end{pmatrix} + \begin{pmatrix} v_x \\ v_y \\ v_z \end{pmatrix} \right)\\
        &= T_c \begin{pmatrix} u_x + v_x \\ u_y + v_y \\ u_z + v_z \end{pmatrix}\\
        &= \begin{pmatrix} u_x + v_x \\ (u_y + v_y)^2 \\ (u_z + v_z)^3 \end{pmatrix},
    \end{align*}
    whereas 
    \begin{align*}
    T_c (\vecu)+T_c(\vecv) &= T_c \begin{pmatrix} u_x \\ u_y \\ u_z \end{pmatrix} +T_c\begin{pmatrix} v_x \\ v_y \\ v_z \end{pmatrix}\\
    &= \begin{pmatrix} u_x \\ u_y^2 \\ u_z^3 \end{pmatrix} + \begin{pmatrix} v_x \\ v_y^2 \\ v_y^3 \end{pmatrix}\\
    &= \begin{pmatrix} u_x + v_x \\ u_y^2 + v_y^2 \\ u_z^3 + v_z^3 \end{pmatrix}.
    \end{align*}
    Note that $u_y^2+v_y^2\neq (u_y+v_y)^2$ and $u_z^3+v_z^3\neq (u_z+v_z)^3$. 
    
    To see that (ii) does not hold, take 
    \begin{align*}
        T_c(\alpha \vecv ) &= T_c\begin{pmatrix} \alpha v_x \\ \alpha v_y \\ \alpha v_z \end{pmatrix}\\
        &= \begin{pmatrix} \alpha v_x \\ \alpha^2 v_y^2 \\ \alpha^3 v_z^3\end{pmatrix},
    \end{align*}
    whereas
    \begin{align*}
        \alpha T_c(\vecv)&= \begin{pmatrix} \alpha v_x \\ \alpha v_y^2 \\ \alpha v_z^3\end{pmatrix}.
    \end{align*}
    These are clearly not equal for every scalar $\alpha$.
    \item This function is linear. For (i), we have
    \begin{align*}
        T_d(\vecu + \vecv) &= T_d \begin{pmatrix} u_x + v_x \\ u_y + v_y \end{pmatrix}\\
        &=\begin{pmatrix} (u_x+v_x)+(u_y+v_y) \\ (u_x+v_x)+(u_y+v_y) \\ (u_x+v_x) + (u_y +v_y) \end{pmatrix}\\
        &= \begin{pmatrix} u_x + u_y \\ u_x + u_y \\ u_x + u_y \end{pmatrix} + \begin{pmatrix} v_x + v_y \\ v_x + v_y \\ v_x + v_y \end{pmatrix}\\
        &= T(\vecu) + T(\vecv).
    \end{align*}
    And for (ii) we have
    \begin{align*}
        T_d(\alpha \vecv) &= T_d \begin{pmatrix} \alpha v_x \\ \alpha v_y \end{pmatrix}\\
        &= \begin{pmatrix} \alpha v_x + \alpha v_y \\ \alpha v_x + \alpha v_y \\ \alpha v_x + \alpha v_y \end{pmatrix}\\
        &= \alpha T_d(\vecv).
    \end{align*}
\end{enumerate}
\end{solution}

\newpage
\begin{problem}
Consider the linear transformation $J \colon R^2 \to \R^2$ defined by 
\[
J(\xhat) = \yhat \qquad \textrm{and} \qquad J(\yhat) = -\xhat.
\]
This linear transformation is fundamental in understanding how we can reconstruct complex numbers using matrices.
\begin{enumerate}[(a)]
    \item Show that $J^2 = J\circ J= -1$.
    \item Determine a matrix representation for $J$ and denote it by $[J]$.
    \item Recall that we can represent a complex number as $z=x + iy$ and that we can represent $z$ as a vector in $\R^2$ as $\vec{\boldsymbol{\zeta}} = x\xhat + y\yhat$.  Show that $J \vec{\boldsymbol{\zeta}}$ corresponds to $iz$.
    \item We can completely reconstruct a representation of $\C$ by using a matrix representation.  In particular, we can take
    \[
        [z] = x [I] + y [J].
    \]
    Show that we recover the complex addition and multiplication using this representation.
    \item We can represent a unit complex number as $z=e^{i\theta}$.  Show that the representation described before leads to
    \[
        [z] = \begin{pmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{pmatrix}.
    \]
\end{enumerate}
\end{problem}
\begin{solution}
    \begin{enumerate}[(a)]
        \item Let $\vecv = v_1 \xhat + v_2 \yhat$ be some arbitrary vector in $\R^2$.  Then,
        \begin{align*}
            J^2(\vecv) = J(J(\vecv)) &= J(J(v_1 \xhat + v_2 \yhat))\\
            &= J(v_1 J(\xhat) + v_2 J(\yhat))\\
            &= J(v_1 \yhat - v_2 \xhat)\\
            &= v_1 J(\yhat) - v_2J(\xhat)\\
            &= -v_1 \xhat - v_2 \yhat\\
            &= -\vecv.
        \end{align*}
        So, yes, $J^2$ acts like scaling by -1.
        \item We determine a matrix for $J$ by using the definition of $J$ on $\xhat$ and $\yhat$. In particular,
        \[
            [J] = \begin{pmatrix} \vert & \vert \\ J(\xhat) & J(\yhat) \\ \vert & \vert \end{pmatrix} = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}.
        \]
        One can check here that $[J]^2=-[I]$, where $[I]$ is the identity matrix. This confirms that $[J]$ satisfies the relationship we saw in $(a)$.
        \item In the complex plane, we let $z=x+iy$ and we can note that
        \[
        iz = -y + ix.
        \]
        Now, we can think of $z$ as a vector in $\R^2$ by noticing that the vector $\vec{\boldsymbol{\zeta}} = x\xhat + y\yhat$ corresponds to the same exact point geometrically.  Then, if we apply $J$ we have
        \[
        J\vec{\boldsymbol{\zeta}} = -y\xhat + x \yhat,
        \]
        which is exactly how $z$ was transformed when we multiplied by $i$. Keep in mind that $i$ rotates a complex number $z$ by $\pi/2$ in the counterclockwise direction and $J$ does the same to vectors $\vec{\boldsymbol{\zeta}}$. To see this most fully, consider drawing a picture of both transformations.
        \item In the complex plane, we can take two complex numbers $z_1 = x_1 + iy_1$ and $z_2 = x_2 + i y_2$. Then we have
        \[
        z_1 + z_2 = (x_1 + x_2) + i(y_1 + y_2) \qquad \textrm{and} \qquad z_1 z_2 = (x_1x_2 - y_1 y_2) + i(x_1 y_2 + x_2 y_1).
        \]
        Notice that addition is componentwise and keep track of this result from the multiplication.

        Now, we can consider two matrices $[z_1] = x_1 [I] + y_1 [J]$ and $[z_2] = x_2 [I] + y_2 [J]$ and see what we get through addition and multiplication. We have
        \[
        [z_1] + [z_2] = (x_1 + x_2)[I] + (y_1 + y_2)[J].
        \]
        This is due to how matrices add componentwise and we can see that this corresponds to the addition in $\C$. Next, we have
        \begin{align*}
        [z_1][z_2] &= (x_1 [I] + y_1 [J])(x_2 [I] + y_2[J]) \\
        &= x_1 x_2 [I]^2 + y_1 x_2 [J][I] + x_1 y_2 [I][J] + y_1 y_2 [J]^2\\
        &= x_1 x_2 [I] + y_1 x_2 [J] + x_1 y_2 [J] - y_1 y_2 [I]\\
        &= (x_1 x_2 - y_1 y_2)[I] + (x_1 y_2 + x_2 y_1)[J].
        \end{align*}
        Note that I use the facts $[J][I]=[I][J]=[J]$, $[I]^2=[I]$, and from (a) we know $[J]^2=-[I]$.  If we take a look at the end result, we can see that this is the same multiplication result as $z_1 z_2$ in $\C$.

        \item Using our knowledge from the previous problem, and Euler's formula, we know that we can take
        \[
        [e^{i\theta}] = \cos (\theta) [I] + \sin(\theta)[J].
        \]
        Writing out the matrices explicitly yields
        \[
        [e^{i\theta}] = \begin{pmatrix} \cos(\theta) & 0 \\ 0 & \cos(\theta) \end{pmatrix} + \begin{pmatrix} 0 & - \sin(\theta) \\ \sin(\theta) & 0 \end{pmatrix} = \begin{pmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{pmatrix},
        \]
        as intended. 
    \end{enumerate}
\begin{remark}
    If one goes to look up a rotation matrix for $\R^2$, you will find the matrix you found in (e).  So, this goes to show that complex arithmetic captures rotations nicely through Euler's formula. Moreover, the matrix representation for a complex number is faithful in describing all that we need.
\end{remark}
\end{solution}

\newpage
\begin{problem}
Write down the matrix for the following linear transformation $T\colon \R^3 \to \R^3$:
\[
T\begin{pmatrix} x\\ y\\ z \end{pmatrix}
= \begin{pmatrix} x+y+z\\ 2x\\ 3y + z \end{pmatrix}.
\]
\end{problem}
\begin{solution}
We need that
\begin{align*}
[T] \begin{pmatrix} x \\ y \\ z \end{pmatrix} &= \begin{pmatrix} x+y+z\\ 2x\\ 3y + z \end{pmatrix}
\end{align*}
via matrix multiplication. Since the input vector is a 3-dimensional vector, and the output vector is $3$-dimensional, we must have that $[T]$ is a $3\times 3$-matrix. Hence,
\[
[T] = \begin{pmatrix} t_{11} & t_{12} & t_{13} \\ t_{21} & t_{22} & t_{23} \\ t_{31} & t_{32} & t_{33} \end{pmatrix}.
\]
Then we have
\begin{align*}
\begin{pmatrix} t_{11} & t_{12} & t_{13} \\ t_{21} & t_{22} & t_{23} \\ t_{31} & t_{32} & t_{33} \end{pmatrix} \begin{pmatrix} x \\ y \\ z \end{pmatrix} &=  \begin{pmatrix} t_{11}x + t_{12}y + t_{13}z \\ t_{21}x +  t_{22}y + t_{23}z \\ t_{31}x + t_{32}y + t_{33}z \end{pmatrix} = \begin{pmatrix} x + y + z \\ 2x \\ 3y+z \end{pmatrix}.
\end{align*}
If we match the coefficients on the $x$, $y$, and $z$, we find that
\[
[T] = \begin{pmatrix} 1 & 1 & 1 \\ 2 & 0 & 0 \\ 0 & 3 & 1 \end{pmatrix}.
\]
\end{solution}


\newpage
\begin{problem}
    Take the following matrices:
    \[
        [A] = \begin{pmatrix} 4 & 3 & 10 & 2 \\ 1 & 1 & 0 & 9 \end{pmatrix},\quad [B] = \begin{pmatrix} 8 & 5 & 8 \\ 10 & 9 & 2 \\ 4 & 6 &3 \end{pmatrix}, \quad [C] = \begin{pmatrix} 0 & 0 & 9 \\ 7 & 9 & 9 \\ 1 & 9 & 9 \\ 3 & 3 & 1\end{pmatrix}
    \]
    \begin{enumerate}[(a)]
        \item Compute either $[A][C]$ or $[C][A]$ and state which multiplication is not possible.
        \item Compute either $[B][C]$ or $[C][B]$ and state which multiplication is not possible.
        \item Can you add any of these matrices?
        \item Describe each matrix as linear transformation $T\colon \R^m \to \R^n$. What is $m$ and $n$ for each? How does this relate to the number of rows and columns?
    \end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item The matrix $[A]$ is a $2\times 4$ matrix and matrix $[C]$ is a $4 \times 3$ matrix.  So we can compute $[A][C]$ but not $[C][A]$.  Given that, we also expect the output to be a $2 \times 3$ matrix. So, we have
    \[
    [A][C] = \begin{pmatrix} 4 & 3 & 10 & 2 \\ 1 & 1 & 0 & 9 \end{pmatrix}\begin{pmatrix} 0 & 0 & 9 \\ 7 & 9 & 9 \\ 1 & 9 & 9 \\ 3 & 3 & 1\end{pmatrix} = \begin{pmatrix} 37 & 123 & 155 \\ 34 & 36 & 27 \end{pmatrix}.
    \]

    \item $[B]$ is a $3 \times 3$ matrix so we can take $[C][B]$ but not $[B][C]$.  We get
    \[
        [C][B] = \begin{pmatrix} 0 & 0 & 9 \\ 7 & 9 & 9 \\ 1 & 9 & 9 \\ 3 & 3 & 1\end{pmatrix} \begin{pmatrix} 8 & 5 & 8 \\ 10 & 9 & 2 \\ 4 & 6 &3 \end{pmatrix} = \begin{pmatrix} 36 & 54 & 27 \\ 182 & 170 & 101 \\ 134 & 140 & 53 \\ 58 & 48 & 33 \end{pmatrix}.
    \]
    \item We can always add a matrix to itself, so, for example $[A]+[A]$, $[B]+[B]$, and $[C]+[C]$ make sense. However, since the dimensions of $[A]$, $[B]$, and $[C]$ all differ, we cannot add in any other way.
    \item The number of columns of a matrix denotes the input dimension $m$, and the number of rows denotes the output dimension $n$.  So
    \[
        A \colon \R^4 \to \R^2, \quad B \colon \R^3 \to \R^3, \quad C \colon \R^3 \to \R^4.
    \]
\end{enumerate}
\end{solution}


\newpage
\begin{problem}
Solve the following equation.
\[
\begin{pmatrix} 1 & 1 & 1 \\ 1 & 2 & 1 \\ 1 & 2 & 2 \end{pmatrix} \begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} 6 \\ 8 \\ 11 \end{pmatrix}.
\]
\end{problem}
\begin{solution}
First, we create the augmented matrix
\[
\left(\begin{tabular}{ccc|c} 1 & 1 & 1 & 6 \\ 1 & 2 & 1 & 8 \\ 1 & 2 & 2 & 11 \end{tabular} \right).
\]
We can subtract R1 from both R2 and R3 to get
\[
\left(\begin{tabular}{ccc|c} 1 & 1 & 1 & 6 \\ 0 & 1 & 0 & 2 \\ 0 & 1 & 1 & 5 \end{tabular} \right).
\]
Then subtract R3 from R1 to get
\[
\left(\begin{tabular}{ccc|c} 1 & 0 & 0 & 1 \\ 0 & 1 & 0 & 2 \\ 0 & 1 & 1 & 5 \end{tabular} \right).
\]
Finally, subtract R2 from R3 to get
\[
\left(\begin{tabular}{ccc|c} 1 & 0 & 0 & 1 \\ 0 & 1 & 0 & 2 \\ 0 & 0 & 1 & 3 \end{tabular} \right).
\]
This yields our result in the right most column in that $x=1$, $y=2$, and $z=3$. 
\end{solution}



\end{document}