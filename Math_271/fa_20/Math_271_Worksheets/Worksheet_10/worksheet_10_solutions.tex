%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document data
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt]{article} %report allows for chapters
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{preamble}

\newcommand{\vecx}{\vec{\boldsymbol{x}}}
\newcommand{\vecy}{\vec{\boldsymbol{y}}}
\newcommand{\vecp}{\vec{\boldsymbol{p}}}
\newcommand{\Null}{\operatorname{Null}}
\newcommand{\evec}{\vec{\boldsymbol{e}}}
\begin{document}

\begin{center}
   \textsc{\large MATH 271, Worksheet 10, \emph{Solutions.}}\\
   \textsc{Inverse and similar matrices. Eigenvalue problem and diagonalization. Hermitian matrices.}
\end{center}
\vspace{.5cm}


\begin{problem}
Consider the two matrices 
\[
[A] = \begin{pmatrix} 3 & 1 \\ 6 & 2 \end{pmatrix} \qquad \textrm{and} \qquad [B] = \begin{pmatrix} 1 & 2\\ 2 & 1 \end{pmatrix}.
\]
\begin{enumerate}[(a)]
    \item Argue why the matrix $[A]$ cannot be invertible.
    \item Compute the inverse matrix $[B]^{-1}$ for $[B]$.  
    \item Solve the system of equations $[B]\vec{\boldsymbol{x}} = \vec{\boldsymbol{y}}$ for the following vectors.
    \begin{enumerate}[i.]
        \item $\vec{\boldsymbol{y}} = \begin{pmatrix} 2 \\ 2 \end{pmatrix}$.
        \item $\vec{\boldsymbol{y}} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$.
        \item $\vec{\boldsymbol{y}} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$.
    \end{enumerate}
\end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item To see $[A]$ is not invertible we note that $\det([A])=0$. The columns of $[A]$ are linearly dependent, so it cannot be inverted.
    \item To find $[B]^{-1}$ we start with the augmented matrix
    \[
        \left( \begin{tabular}{cc|cc} 1 & 2 & 1 & 0 \\ 2 & 1 & 0  & 1 \end{tabular} \right).
    \]
    We can subtract 2R1 from R2 to get
    \[
        \left( \begin{tabular}{cc|cc} 1 & 2 & 1 & 0 \\ 0 & -3 & -2  & 1 \end{tabular} \right),
    \]
    and then we can multiply R3 by -1/3 to get
    \[
        \left( \begin{tabular}{cc|cc} 1 & 2 & 1 & 0 \\ 0 & 1 & 2/3  & -1/3 \end{tabular} \right).
    \]
    We can now subtract 2R2 from R1 to get 
    \[
        \left( \begin{tabular}{cc|cc} 1 & 0 & -1/3 & 2/3 \\ 0 & 1 & 2/3  & -1/3 \end{tabular} \right)
    \]
    which tells us
    \[
    [B]^{-1} = \begin{pmatrix} -1/3 & 2/3 \\ 2/3 & -1/3 \end{pmatrix}.
    \]
    \item We can note that if $[B]\vecx = \vecy$ then we also have $\vecx = [B]^{-1} \vecy$.  Thus, we find the following.
    \begin{enumerate}[i.]
        \item With this $\vecy$ we get
        \[
        \vecx = \begin{pmatrix} -1/3 & 2/3 \\ 2/3 & -1/3 \end{pmatrix} \begin{pmatrix} 2 \\ 2 \end{pmatrix} = \begin{pmatrix} 2/3 \\ 2/3 \end{pmatrix}.
        \]
        \item Next, we have
        \[
        \vecx = \begin{pmatrix} -1/3 & 2/3 \\ 2/3 & -1/3 \end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 2/3 \\ 2/3 \end{pmatrix}.
        \]
        \item Finally,
        \[
        \vecx = \begin{pmatrix} -1/3 & 2/3 \\ 2/3 & -1/3 \end{pmatrix} \begin{pmatrix} 0 \\ 0 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}.
        \]        
    \end{enumerate}
\end{enumerate}
\end{solution}

\newpage
\begin{problem}
Consider the matrices 
\[
[A] = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix} \qquad \textrm{and} \qquad [B] = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}.
\]
\begin{enumerate}[(a)]
    \item Show that $[A]$ and $[B]$ are both invertible.
    \item Find $[A]^{-1}$ and $[B]^{-1}$.
    \item Show that $([A][B])^{-1} = [B]^{-1} [A]^{-1}$.
\end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item To see both matrices are invertible we note that $\det([A])=1$ and $\det([B])=2$ are both nonzero.
    \item To find the inverses, we can row reduce. The work for $[A]$ is quick. We take the augmented matrix
    \[
        \left( \begin{tabular}{cc|cc} 1 & 1 & 1 & 0 \\ 0 & 1 & 0  & 1 \end{tabular} \right).
    \]
    We subtract R2 from R1 to get
    \[
        \left( \begin{tabular}{cc|cc} 1 & 0 & 1 & -1 \\ 0 & 1 & 0  & 1 \end{tabular} \right),
    \]
    showing that
    \[
        [A]^{-1} = \begin{pmatrix} 1 & -1 \\ 0 & 1 \end{pmatrix}.
    \]
    Similarly, one can show
    \[
        [B]^{-1} = \begin{pmatrix} 2/3 & -1/3 \\ -1/3 & 2/3 \end{pmatrix},
    \]
    and the work is very similar to Problem 1.
    \item We have
    \[
    [A][B] = \begin{pmatrix} 3 & 3 \\ 1 & 2 \end{pmatrix}.
    \]
    Then we also have
    \[
    [B]^{-1} [A]^{-1} = \begin{pmatrix} 2/3 & -1 \\ 1/3 & 1 \end{pmatrix}.
    \]
    If we multiply those two matrices together, we should get the identity $[I]$. Indeed, we have
    \[
    ([A][B])([B]^{-1} [A]^{-1}) = \begin{pmatrix} 3 & 3 \\ 1 & 2 \end{pmatrix}\begin{pmatrix} 2/3 & -1 \\ 1/3 & 1 \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}.
    \]
\end{enumerate}
\end{solution}

\newpage
\begin{problem}
Simplify the following expressions.
\begin{enumerate}[(a)]
    \item $([A][B])^{-1} [A] [B]$.
    \item $[A]^2[B]^3[A] ([A][B])^{-1}$.
    \item $([A][B][C]^{-1})^{-1} [A][B] [C]^{-1}$.
\end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item We have
    \begin{align*}
        ([A][B])^{-1} [A][B] &= [B]^{-1} [A]^{-1} [A][B]\\
        &= [B]^{-1} [I] [B]\\
        &= [B]^{-1} [B]\\
        &= [I].
    \end{align*}
    \item We have
    \begin{align*}
        [A]^2[B]^3[A] ([A][B])^{-1} &= [A]^2 [B]^3 [A] [B]^{-1} [A],
    \end{align*}
    which cannot be further simplified.
    \item We have
    \begin{align*}
        ([A][B][C]^{-1})^{-1} [A][B][C]^{-1} &= [C][B]^{-1}[A]^{-1} [A][B][C]^{-1}\\
        &= [I].
    \end{align*}
\end{enumerate}
\end{solution}

\newpage
\begin{problem}
Show that for any invertible matrix $[A]$ that $\det([A]^{-1})=\frac{1}{\det([A])}$.
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item We have
    \[
    1=\det([I]) = \det([A][A]^{-1}) = \det([A])\det([A]^{-1}).
    \]
    Thus, it must be that
    \[
    \det([A]^{-1}) = \frac{1}{\det([A])}.
    \]
\end{enumerate}
\end{solution}

\newpage
\begin{problem}
Let $[B]$ be similar to $[A]$ by the relationship $[B]=[P]^{-1} [A] [P]$.
\begin{enumerate}[(a)]
    \item Given that $[P]$ is invertible, show that $[P]$ transforms the standard basis $\xhat_1,\xhat_2,\dots,\xhat_n$ into a new basis given by the columns of $[P]$.
    \item Show that $[P]^{-1}$ transforms the basis given by the columns of $[P]$ into the standard basis.
    \item Explain why $[B]$ performs the same transformation as $[A]$ but just on a different basis (e.g., different choices of coordinates).
\end{enumerate}
\end{problem}
\begin{solution}~
    \begin{enumerate}[(a)]
        \item We know that $[P]$ is square, so let's say $[P]$ is $n\times n$. Let the $i$th column of $[P]$ be given by the vector $\vecp_i$ so that
        \[
        [P] = \begin{pmatrix} \vert & \vert & ~ & \vert \\ \vecp_1 & \vecp_2 & \cdots & \vecp_n \\ \vert & \vert & ~ & \vert \end{pmatrix}.
        \]
        Now, by construction, we have
        \[
        [P] \xhat_i = \vecp_i.
        \]
        \item Since $[P]$ is invertible, we left multiply the above equation by $[P]^{-1}$ and we get
        \[
        \xhat_i = [P]^{-1} \vecp_i.
        \]
        \item Since $[B]= [P]^{-1} [A] [P]$ we have
        \[
        [P][B] = [A] [P].
        \]
        Thus, for the standard basis vector $\xhat_i$ we get
        \[
        [P][B] \xhat_i = [A] \vecp_i,
        \]
        from the above equation. $[B]\xhat_i$ gives us a linear combination in the standard basis and we then multiply by $[P]$ to transform the standard basis to the basis given by the vectors $\vecp_i$.  So we see that $[B]$ transforms $\xhat_i$ vectors in the same way that $[A]$ transforms the $\vecp_i$ vectors.
    \end{enumerate} 
\end{solution}

\newpage
\begin{problem}
Let $[B]$ be similar to $[A]$ by the relationship $[B]=[P]^{-1} [A] [P]$.  
\begin{enumerate}[(a)]
    \item Show that the trace is invariant under similarity. That is, show $\tr ([A]) = \tr([B])$.
    \item Show that the determinant is invariant under similarity. \emph{Hint: you will need to use the result from Problem 4.}
    \item Show that $[A]$ and $[B]$ have the same eigenvalues. It may help to think that if we have $\vecv$ as an eigenvector for $[A]$, then what is the corresponding eigenvector for $[B]$?
\end{enumerate}
\end{problem}
\begin{solution}~
    \begin{enumerate}[(a)]
        \item We have
        \[
        \tr([B])=\tr([P]^{-1}[A][P]) = \tr([A][P][P]^{-1}) = \tr([A]).
        \]
        \item Similarly,
        \[
        \det([B])=\det([P]^{-1} [A] [P]) = \det([P]^{-1}) \det([A]) \det([P]) = \det([A]),
        \]
        once we note $\det([P]^{-1}) = \frac{1}{\det([P])}$.
        \item Let $\vecv$ be an eigenvector of $[B]$ with eigenvalue $\lambda$ which means
        \[
        [B]\vecv = \lambda \vecv.
        \]
        Thus, we must have
        \[
        [P]^{-1} [A] [P] \vecv = \lambda \vecv.
        \]
        In particular,
        \[
        [A] [P] \vecv = \lambda [P]\vecv,
        \]
        and we realize $[P]\vecv$ is the eigenvector of $[A]$ with eigenvalue $\lambda$.
    \end{enumerate}
\end{solution}

\newpage
\begin{problem}
    Compute the eigenvalues and eigenvectors for the following matrices.
\begin{enumerate}[(a)]
    \item $[A] = \begin{pmatrix} 2 & 1 \\ 0 & 3 \end{pmatrix}$.
    \item $[B] = \begin{pmatrix} 1 & 3 \\ 5 & 1 \end{pmatrix}$.
    \item $[C] = \begin{pmatrix} 1 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 1 \end{pmatrix}$.
\end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item To find the eigenvalues of $[A]$ we first find the characteristic polynomial $\det([A]-\lambda [I])$. We have
    \[
        [A]-\lambda [I] = \begin{pmatrix} 2 & 1 \\ 0 & 3 \end{pmatrix} - \begin{pmatrix} \lambda & 0 \\ 0 & \lambda \end{pmatrix} = \begin{pmatrix} 2-\lambda & 1 \\ 0  & 3 - \lambda \end{pmatrix}.
    \]
    Thus, the characteristic polynomial is $\det([A]-\lambda [I]) = (2-\lambda)(3-\lambda)$.  The roots to the characteristic polynomial are the eigenvalues so we have
    \[
        \lambda_1 = 2 \qquad \textrm{and} \qquad \lambda_2 = 3.
    \]
    We can now find the corresponding eigenvectors.

    \noindent\textbf{For $\lambda_1 = 2$:} We note the eigenvectors are elements of $\Null([A]-\lambda_1[I])$.  We have
    \[
    [A]-\lambda_1 [I] = \begin{pmatrix} 0 & 1 \\ 0 & 1 \end{pmatrix}.
    \]
    We get the augmented matrix
    \[
        \left(\begin{tabular}{cc|c} 0 & 1 & 0 \\ 0 & 1 & 0 \end{tabular}\right),
    \]
    and we can take subtract R1 from R2 to get
    \[
        \left(\begin{tabular}{cc|c} 0 & 1 & 0 \\ 0 & 0 & 0 \end{tabular}\right).
    \]
    This corresponds to two equations
    \begin{align*}
        0x + y &=0\\
        0x + 0y &=0.
    \end{align*}
    Thus, we must have $y=0$, but $x$ is free to be anything.  Note that $\zerovec$ is never an eigenvector (we don't allow it) but we can simply choose $x=1$ and get the eigenvector
    \[
    \evec_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix}.
    \]

    \noindent\textbf{For $\lambda_2 = 3$:} We note the eigenvectors are elements of $\Null([A]-\lambda_2[I])$.  We have
    \[
    [A]-\lambda_2 [I] = \begin{pmatrix} -1 & 1 \\ 0 & 0 \end{pmatrix}.
    \]
    We get the augmented matrix
    \[
        \left(\begin{tabular}{cc|c} -1 & 1 & 0 \\ 0 & 0 & 0 \end{tabular}\right).
    \]
    This corresponds to two equations
    \begin{align*}
        -x + y &=0\\
        0x + 0y &=0.
    \end{align*}
    So, $x$ and $y$ are free to be anything so long as $x=y$. Choose $x=1$ so that $y=1$ as well and the eigenvector is
    \[
    \evec_2 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}.
    \]

    \item The process is the same.  We have
    \[
    \det([B]-\lambda [I]) = (1-\lambda)(1-\lambda)-15.
    \]
    The roots to this polynomial are then
    \[
    \lambda_1 = 1-\sqrt{15} \qquad \textrm{and} \qquad \lambda_2 = 1+ \sqrt{15}.
    \]

    \noindent\textbf{For $\lambda_1 = 1-\sqrt{15}$:} We note the eigenvectors are elements of $\Null([B]-\lambda_1[I])$.  We have
    \[
    [B]-\lambda_1 [I] = \begin{pmatrix} \sqrt{15} & 3 \\ 5 & \sqrt{15} \end{pmatrix}.
    \]
    We get the augmented matrix
    \[
        \left(\begin{tabular}{cc|c} $\sqrt{15}$ & 3 & 0 \\ 5 & $\sqrt{15}$ & 0 \end{tabular}\right),
    \]
    which can be reduced to
    \[
        \left(\begin{tabular}{cc|c} $1$ & $\sqrt{\frac{3}{5}}$ & 0 \\ 0 & 0 & 0 \end{tabular}\right),
    \]
    and so we see that $x=-\sqrt{\frac{3}{5}}y$. If we choose $y=1$ then $x=-\sqrt{\frac{3}{5}}$ giving us the eigenvector
    \[
    \evec_1 = \begin{pmatrix} -\sqrt{\frac{3}{5}} \\ 1 \end{pmatrix}.
    \]

    \noindent\textbf{For $\lambda_2 = 1+\sqrt{15}$:} We note the eigenvectors are elements of $\Null([B]-\lambda_1[I])$.  We have
    \[
    [B]-\lambda_2 [I] = \begin{pmatrix} -\sqrt{15} & 3 \\ 5 & -\sqrt{15} \end{pmatrix}.
    \]
    We get the augmented matrix
    \[
        \left(\begin{tabular}{cc|c} $-\sqrt{15}$ & 3 & 0 \\ 5 & $-\sqrt{15}$ & 0 \end{tabular}\right),
    \]
    which can be reduced to
    \[
        \left(\begin{tabular}{cc|c} $1$ & $-\sqrt{\frac{3}{5}}$ & 0 \\ 0 & 0 & 0 \end{tabular}\right),
    \]
    and so we see that $x=\sqrt{\frac{3}{5}}y$. If we choose $y=1$ then $x=\sqrt{\frac{3}{5}}$ giving us the eigenvector
    \[
    \evec_2 = \begin{pmatrix} \sqrt{\frac{3}{5}} \\ 1 \end{pmatrix}.
    \]

    \item Note that $[C]$ is real and symmetric, so we expect the eigenvalues to be real and the eigenvectors to be orthogonal. Indeed, we have the characteristic polynomial
    \[
    \det([C]-\lambda [I]) = -\lambda^3 + 2\lambda^2 + \lambda -2,
    \]
    which as roots
    \[
    \lambda_1 = -1, \quad \lambda_2 = 1, \quad \lambda_3 = 2.
    \]
    These roots are the eigenvalues and each is real.

    \noindent\textbf{For $\lambda_1 = -1$:} We note the eigenvectors are elements of $\Null([B]-\lambda_1[I])$.  We have
    \[
    [C]-\lambda_1 [I] = \begin{pmatrix} 2 & 1 & 0 \\ 1 & 1 & 1 \\ 0 & 1 & 2 \end{pmatrix}.
    \]
    We get the augmented matrix
    \[
        \left(\begin{tabular}{ccc|c} 2 & 1 & 0 & 0 \\ 1 & 1 & 1 & 0 \\ 0 & 1 & 2 & 0 \end{tabular}\right),
    \]
    which can be reduced to
    \[
        \left(\begin{tabular}{ccc|c} 1 & 0 & -1 & 0 \\ 0 & 1 & 2 & 0 \\ 0 & 0 & 0 & 0 \end{tabular}\right),
    \]
    and we note $x=z$ and $y=-2z$. If we then choose $z=1$ then $x=1$ and $y=2$ giving us the eigenvector
    \[
    \evec_1 = \begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix}.
    \]

    \noindent\textbf{For $\lambda_2 = 1$:} We note the eigenvectors are elements of $\Null([B]-\lambda_2[I])$.  We have
    \[
    [C]-\lambda_2 [I] = \begin{pmatrix} 0 & 1 & 0 \\ 1 & -1 & 1 \\ 0 & 1 & 0 \end{pmatrix}.
    \]
    We get the augmented matrix
    \[
        \left(\begin{tabular}{ccc|c} 0 & 1 & 0 & 0 \\ 1 & -1 & 1 & 0 \\ 0 & 1 & 0 & 0 \end{tabular}\right),
    \]
    which can be reduced to
    \[
        \left(\begin{tabular}{ccc|c} 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{tabular}\right),
    \]
    and we note $x=-z$ and $y=0$. If we then choose $x=1$ then $z=-1$ giving us the eigenvector
    \[
    \evec_2 = \begin{pmatrix} 1 \\ 0 \\ -1 \end{pmatrix}.
    \]

    \noindent\textbf{For $\lambda_3 = 2$:} We note the eigenvectors are elements of $\Null([B]-\lambda_3[I])$.  We have
    \[
    [C]-\lambda_3 [I] = \begin{pmatrix} -1 & 1 & 0 \\ 1 & -2 & 1 \\ 0 & 1 & -1 \end{pmatrix}.
    \]
    We get the augmented matrix
    \[
        \left(\begin{tabular}{ccc|c} -1 & 1 & 0 & 0 \\ 1 & -2 & 1 & 0 \\ 0 & 1 & -1 & 0 \end{tabular}\right),
    \]
    which can be reduced to
    \[
        \left(\begin{tabular}{ccc|c} 1 & 0 & -1 & 0 \\ 0 & 1 & -1 & 0 \\ 0 & 0 & 0 & 0 \end{tabular}\right),
    \]
    and we note $x=z$ and $y=-z$. If we then choose $x=1$ then $z=1$ and $y=-1$ giving us the eigenvector
    \[
    \evec_3 = \begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix}.
    \]

    One can then show that $\evec_1$, $\evec_2$, and $\evec_3$ are orthogonal!
\end{enumerate}
\end{solution}

\newpage
\begin{problem}
Diagonalize the above matrices (if possible).
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item Our eigenvectors for $[A]$ are $\evec_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$ and $\evec_2 = \begin{pmatrix} 1 \\ 1\end{pmatrix}$. Thus we can take our $[P]$ matrix as
    \[
    [P] = \begin{pmatrix} \vert & \vert \\ \evec_1 & \evec_2 \\ \vert & \vert\end{pmatrix} = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}.
    \]
    Then we have
    \[
    [P]^{-1} = \begin{pmatrix} 1 & -1 \\ 0 & 1 \end{pmatrix}.
    \]
    Thus we can find a matrix $[D]$ similar to $[A]$ that is diagonal by
    \[
    [D] = [P]^{-1} [A] [P] = \begin{pmatrix} 1 & -1 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 2 & 1 \\ 0 & 3 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix} = \begin{pmatrix} 2 & 0 \\ 0 & 3 \end{pmatrix}.
    \]
    Note that the eigenvalues are in order along the diagonal based on how we ordered the eigenvectors in $[P]$.

    \item Repeating now for $[B]$ we take
    \[
    [P] = \begin{pmatrix} \vert & \vert \\ \evec_1 & \evec_2 \\ \vert & \vert\end{pmatrix} = \begin{pmatrix} -\sqrt{\frac{3}{5}} & 1 \\ \sqrt{\frac{3}{5}} & 1 \end{pmatrix},
    \]
    and
    \[
    [P]^{-1} = \begin{pmatrix} -\frac{\sqrt{15}}{6} & \frac{1}{2} \\ \frac{\sqrt{15}}{6} & \frac{1}{2} \end{pmatrix}
    \]
    Then
    \[
    [D] = [P]^{-1} [B] [P] = \begin{pmatrix} 1-\sqrt{15} & 0 \\ 0 & 1+\sqrt{15} \end{pmatrix}.
    \]

    \item Finally for $[C]$ we have
    \[
    [P] = \begin{pmatrix} \vert & \vert & \vert \\ \evec_1 & \evec_2 & \evec_2 \\ \vert & \vert & \vert \end{pmatrix} = \begin{pmatrix} 1 & 1 & 1 \\ -2 & 0 & -1 \\ 1 & -1 & 1 \end{pmatrix},
    \]
    and
    \[
    [P]^{-1} = \begin{pmatrix} -\frac{1}{2} & -1 & - \frac{1}{2} \\ \frac{1}{2} & 0 & -\frac{1}{2} \\ 1 & 1 & 1 \end{pmatrix}.
    \]
    Then
    \[
    [D] = [P]^{-1} [B] [P] = \begin{pmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 2 \end{pmatrix}.
    \]
\end{enumerate}
\end{solution}

\newpage
\begin{problem}
Argue why the eigenvectors corresponding to a zero eigenvalue are elements of the nullspace.
\end{problem}
\begin{solution}
This is tautologically true.  Let $A$ be some linear transformation and let $\evec$ be an eigenvector with eigenvalue $\lambda=0$.  Then
\[
A\evec = \lambda \evec = 0\evec = \zerovec.
\]
So we also have $\evec \in \Null(A)$.
\end{solution}

\newpage
\begin{problem}
Show that there must be at least one zero eigenvalue if the determinant of a matrix is zero. Explain what this means geometrically and relate it beck to the geometric interpretation of the determinant.
\end{problem}
\begin{solution}
Consider an $n\times n$ matrix $[A]$ then we have $n$ (possibly repeated and complex) eigenvalues $\lambda_1,\dots,\lambda_n$. We note that $\det([A])$ is the product of the eigenvalues so
\[
\det([A])=\lambda_1 \lambda_2 \cdots \lambda_n.
\]
Thus, the only possible way the determinant can be zero is if at least one $\lambda_i=0$.

To see this geometrically, we note that for $\lambda_i=0$ we have (at least one) corresponding eigenvector $\evec_i$ satisfying $[A]\evec_i = \lambda_i \evec = \zerovec$.  We see now that the linear transformation ``squishes" down $\operatorname{Span}\{\evec\}$ to $\zerovec$. For example, in 3-dimensions we have the transformation we've described before given by
\[
[A] = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix}
\]
for which $\det([A])=0$ and $\zhat$ is an eigenvector with eigenvalue zero.  We note that for any vector $\vecv\in \operatorname{Span}\{\zhat\}$ we can put $\vecv = \alpha \zhat$ and find 
\[
[A]\vecv = [A]\alpha \zhat = \alpha [A]\zhat = 0.
\]
The whole $z$-axis is squished down to zero, and all vectors in $\R^3$ are sent to their ``shadow" in the $xy$-plane.  Any matrix that has a zero eigenvalue will have a direction squished like this!
\end{solution}

\newpage
\begin{problem}
Given the matrix
\[
[A] = \begin{pmatrix} 2 & 1 \\ 0 & 2 \end{pmatrix}.
\]
\begin{enumerate}[(a)]
    \item Using the definition of the adjoint and hermitian (self-adjoint), show that $[A]$ is not hermitian.
    \item Show that there exists only one eigenvector for $[A]$ (e.g., one linearly independent vector in $\operatorname{Null}([A]-\lambda[I])$.
    \item Show that there exists two linearly independent vectors in $\operatorname{Null}(([A]-\lambda [I])^2)$. 
\end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item Let $\vecu,\vecv\in \C^2$ be complex vectors given by $\vecu=u_1\xhat+u_2\yhat$ and $\vecv=v_1\xhat+v_2\yhat$ with $u_1,u_2,v_1,v_2\in \C$.  We first note
    \begin{align*}
    \langle [A]\vecu,\vecv\rangle &= \langle (2u_1 + u_2)\xhat + 2u_2\yhat,v_1\xhat + v_2\yhat \rangle\\
    &= (2u_1+u_2)v_1^* + 2u_2v_2^*.
    \end{align*}
    Now, if $[A]$ was hermitian we'd have $\langle [A]\vecu,\vecv\rangle = \langle \vecu,[A]\vecv\rangle$. Instead,
    \begin{align*}
    \langle \vecu,[A]\vecv \rangle &= \langle u_1\xhat + u_2\yhat,(2v_1+v_2)\xhat+2v_2 \yhat \rangle\\
    &= u_1(2v_1^*+v_2^*)+2u_2v_2^*,
    \end{align*}
    which is not equal to $\langle [A]\vecu,\vecv\rangle$ in general.

    \item We first find the eigenvalue by 
    \[
    \det([A]-\lambda [I])= (2-\lambda)^2.
    \]
    So the eigenvalue is $\lambda=2$. Now we find the eigenvectors which are in $\Null([A]-\lambda[I])$ by taking the augmented matrix for $[A]-\lambda[I]$
    \[
    \left( \begin{tabular}{cc|c} 0 & 1 & 0 \\ 0 & 0 & 0 \end{tabular}\right).
    \]
    We note that $y=0$ and we can choose $x=1$ so the only eigenvector is 
    \[
    \evec = \begin{pmatrix} 1 \\ 0 \end{pmatrix}.
    \]

    \item We have
    \[
    ([A]-\lambda [I])^2 = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}\begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} = \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix}.
    \]
    We can note that $\xhat$ and $\yhat$ are both in $\Null(([A]-\lambda [I])^2)$ and they are linearly independent. 
\end{enumerate}
\end{solution}

\end{document}
