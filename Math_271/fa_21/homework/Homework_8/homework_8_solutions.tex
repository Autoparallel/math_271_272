%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document data
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt]{article} %report allows for chapters
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{preamble}
\newcommand{\vhat}{\boldsymbol{\hat{v}}}
\newcommand{\ehat}{\boldsymbol{\hat{e}}}
\newcommand{\Span}{\operatorname{Span}}

\begin{document}

\begin{center}
   \textsc{\large MATH 271, Homework 8, \emph{Solutions}}\\
\end{center}
\vspace{.5cm}



\begin{problem}
Consider the following vectors in space $\R^3$
\[
\vecu = \ehat_1 + 2\ehat_2 + 3\ehat_3 \qquad \textrm{and} \qquad \vecv = -2\ehat_1 +\ehat_2 -2\ehat_3.
\]
\begin{enumerate}[(a)]
    \item Compute the dot product $\vecu\cdot \vecv$.
    \item Compute the lengths $|\vecu|$ and $|\vecv|$ using the dot product.
    \item Compute the projection of $\vecu$ in the direction of $\vecv$.
    \emph{Hint: don't forget to normalize the vectors before you build your projection.}
    \item Compute the cross product $\vecu \times \vecv$.
    \item Find the area of the parallelogram generated by $\vecu$ and $\vecv$.
\end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item We have that
    \begin{align*}
        \vecu \cdot \vecv &= 1\cdot (-2) + 2 \cdot 1 + 3 \cdot (-3)\\
        &= -6.
    \end{align*}
    \item We compute the lengths using the dot product by
    \[
        \|\vecu\| = \sqrt{\vecu \cdot \vecu} = \sqrt{1^2+2^2+3^2} = \sqrt{14}.
    \]
    Likewise
    \[
        \|\vecv\| = \sqrt{\vecv \cdot \vecv} = \sqrt{(-2)^2 + 1^2 (-2)^2} = \sqrt{9} = 3.
    \]

    \item The projection of $\vecu$ in the direction of $\vecv$ is simply asking for how much of the vector $\vecu$ is in the direction of $\vecv$.  One can arrive at this purely through trigonometry, but we have the dot product at our disposal.  The normalized vector $\vhat$ points in the direction of $\vecv$ with length 1 and
    \[
        \vhat = \frac{1}{\|\vecv\|} \vecv = \frac{1}{3} \vecv.
    \]
    Then, the projection can be computed by
    \[
        \vecu \cdot \vhat = \frac{1}{3} \vecu \cdot \vecv = -2.
    \]
    One should attempt to recover this notion by doing some trigonometry.
 \item Here, feel free to use a formula for a cross product instead of writing it all out.  We will find that
    \begin{align*}
        \vecu \times \vecv &= -7\xhat -4\yhat +5\zhat.
    \end{align*}
        \item The area of the parallelogram is given by the magnitude of the cross product so
    \[
    A = | \vecu \times \vecv | = 3\sqrt{10}.
    \]

\end{enumerate}
\end{solution}


\newpage
\begin{problem}
Write down the matrix for the following linear transformation $T\colon \R^3 \to \R^3$:
\[
T\begin{pmatrix} x\\ y\\ z \end{pmatrix}
= \begin{pmatrix} x+y+z\\ 2x\\ 3y + z \end{pmatrix}.
\]
\end{problem}
\begin{solution}
We need that
\begin{align*}
[T] \begin{pmatrix} x \\ y \\ z \end{pmatrix} &= \begin{pmatrix} x+y+z\\ 2x\\ 3y + z \end{pmatrix}
\end{align*}
via matrix multiplication. Since the input vector is a 3-dimensional vector, and the output vector is $3$-dimensional, we must have that $[T]$ is a $3\times 3$-matrix. Hence,
\[
[T] = \begin{pmatrix} t_{11} & t_{12} & t_{13} \\ t_{21} & t_{22} & t_{23} \\ t_{31} & t_{32} & t_{33} \end{pmatrix}.
\]
Then we have
\begin{align*}
\begin{pmatrix} t_{11} & t_{12} & t_{13} \\ t_{21} & t_{22} & t_{23} \\ t_{31} & t_{32} & t_{33} \end{pmatrix} \begin{pmatrix} x \\ y \\ z \end{pmatrix} &=  \begin{pmatrix} t_{11}x + t_{12}y + t_{13}z \\ t_{21}x +  t_{22}y + t_{23}z \\ t_{31}x + t_{32}y + t_{33}z \end{pmatrix} = \begin{pmatrix} x + y + z \\ 2x \\ 3y+z \end{pmatrix}.
\end{align*}
If we match the coefficients on the $x$, $y$, and $z$, we find that
\[
[T] = \begin{pmatrix} 1 & 1 & 1 \\ 2 & 0 & 0 \\ 0 & 3 & 1 \end{pmatrix}.
\]
\end{solution}

\newpage
\begin{problem}
Consider the linear transformation $J \colon R^2 \to \R^2$ defined by
\[
J(\ehat_1) = \ehat_2 \qquad \textrm{and} \qquad J(\ehat_2) = -\ehat_1.
\]
This linear transformation is fundamental in understanding how we can reconstruct complex numbers using matrices.
\begin{enumerate}[(a)]
    \item Show that $J^2 = J\circ J= -1$.
    \item Determine a matrix representation for $J$ and denote it by $[J]$.
    \item Recall that we can represent a complex number as $z=x + iy$ and that we can represent $z$ as a vector in $\R^2$ as $\vec{\boldsymbol{\zeta}} = x\ehat_1 + y\ehat_2$.  Show that $J \vec{\boldsymbol{\zeta}}$ corresponds to $iz$. \emph{Hint: just show the multiplcations are analogous.}
    \item We can completely reconstruct a representation of $\C$ by using a matrix representation.  In particular, we can take
    \[
        [z] = x [I] + y [J].
    \]
    Show that we recover the complex addition and multiplication using this representation.
    \item We can represent a unit complex number as $z=e^{i\theta}$.  Show that the representation described before leads to
    \[
        [z] = \begin{pmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{pmatrix}.
    \]
\end{enumerate}
\end{problem}
\begin{solution}
    \begin{enumerate}[(a)]
        \item Let $\vecv = v_1 \xhat + v_2 \yhat$ be some arbitrary vector in $\R^2$.  Then,
        \begin{align*}
            J^2(\vecv) = J(J(\vecv)) &= J(J(v_1 \xhat + v_2 \yhat))\\
            &= J(v_1 J(\xhat) + v_2 J(\yhat))\\
            &= J(v_1 \yhat - v_2 \xhat)\\
            &= v_1 J(\yhat) - v_2J(\xhat)\\
            &= -v_1 \xhat - v_2 \yhat\\
            &= -\vecv.
        \end{align*}
        So, yes, $J^2$ acts like scaling by -1.
        \item We determine a matrix for $J$ by using the definition of $J$ on $\xhat$ and $\yhat$. In particular,
        \[
            [J] = \begin{pmatrix} \vert & \vert \\ J(\xhat) & J(\yhat) \\ \vert & \vert \end{pmatrix} = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}.
        \]
        One can check here that $[J]^2=-[I]$, where $[I]$ is the identity matrix. This confirms that $[J]$ satisfies the relationship we saw in $(a)$.
        \item In the complex plane, we let $z=x+iy$ and we can note that
        \[
        iz = -y + ix.
        \]
        Now, we can think of $z$ as a vector in $\R^2$ by noticing that the vector $\vec{\boldsymbol{\zeta}} = x\xhat + y\yhat$ corresponds to the same exact point geometrically.  Then, if we apply $J$ we have
        \[
        J\vec{\boldsymbol{\zeta}} = -y\xhat + x \yhat,
        \]
        which is exactly how $z$ was transformed when we multiplied by $i$. Keep in mind that $i$ rotates a complex number $z$ by $\pi/2$ in the counterclockwise direction and $J$ does the same to vectors $\vec{\boldsymbol{\zeta}}$. To see this most fully, consider drawing a picture of both transformations.
        \item In the complex plane, we can take two complex numbers $z_1 = x_1 + iy_1$ and $z_2 = x_2 + i y_2$. Then we have
        \[
        z_1 + z_2 = (x_1 + x_2) + i(y_1 + y_2) \qquad \textrm{and} \qquad z_1 z_2 = (x_1x_2 - y_1 y_2) + i(x_1 y_2 + x_2 y_1).
        \]
        Notice that addition is componentwise and keep track of this result from the multiplication.

        Now, we can consider two matrices $[z_1] = x_1 [I] + y_1 [J]$ and $[z_2] = x_2 [I] + y_2 [J]$ and see what we get through addition and multiplication. We have
        \[
        [z_1] + [z_2] = (x_1 + x_2)[I] + (y_1 + y_2)[J].
        \]
        This is due to how matrices add componentwise and we can see that this corresponds to the addition in $\C$. Next, we have
        \begin{align*}
        [z_1][z_2] &= (x_1 [I] + y_1 [J])(x_2 [I] + y_2[J]) \\
        &= x_1 x_2 [I]^2 + y_1 x_2 [J][I] + x_1 y_2 [I][J] + y_1 y_2 [J]^2\\
        &= x_1 x_2 [I] + y_1 x_2 [J] + x_1 y_2 [J] - y_1 y_2 [I]\\
        &= (x_1 x_2 - y_1 y_2)[I] + (x_1 y_2 + x_2 y_1)[J].
        \end{align*}
        Note that I use the facts $[J][I]=[I][J]=[J]$, $[I]^2=[I]$, and from (a) we know $[J]^2=-[I]$.  If we take a look at the end result, we can see that this is the same multiplication result as $z_1 z_2$ in $\C$.

        \item Using our knowledge from the previous problem, and Euler's formula, we know that we can take
        \[
        [e^{i\theta}] = \cos (\theta) [I] + \sin(\theta)[J].
        \]
        Writing out the matrices explicitly yields
        \[
        [e^{i\theta}] = \begin{pmatrix} \cos(\theta) & 0 \\ 0 & \cos(\theta) \end{pmatrix} + \begin{pmatrix} 0 & - \sin(\theta) \\ \sin(\theta) & 0 \end{pmatrix} = \begin{pmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{pmatrix},
        \]
        as intended.
    \end{enumerate}
\begin{remark}
    If one goes to look up a rotation matrix for $\R^2$, you will find the matrix you found in (e).  So, this goes to show that complex arithmetic captures rotations nicely through Euler's formula. Moreover, the matrix representation for a complex number is faithful in describing all that we need.
\end{remark}
\end{solution}

\newpage
\begin{problem}
    Take the following matrices:
    \[
        [A] = \begin{pmatrix} 4 & 3 & 10 & 2 \\ 1 & 1 & 0 & 9 \end{pmatrix},\quad [B] = \begin{pmatrix} 8 & 5 & 8 \\ 10 & 9 & 2 \\ 4 & 6 &3 \end{pmatrix}, \quad [C] = \begin{pmatrix} 0 & 0 & 9 \\ 7 & 9 & 9 \\ 1 & 9 & 9 \\ 3 & 3 & 1\end{pmatrix}
    \]
    \begin{enumerate}[(a)]
        \item Compute either $[A][C]$ or $[C][A]$ and state which multiplication is not possible.
        \item Compute either $[B][C]$ or $[C][B]$ and state which multiplication is not possible.
        \item Can you add any of these matrices?
        \item Describe each matrix as linear transformation $T\colon \R^m \to \R^n$. What is $m$ and $n$ for each? How does this relate to the number of rows and columns?
    \end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item The matrix $[A]$ is a $2\times 4$ matrix and matrix $[C]$ is a $4 \times 3$ matrix.  So we can compute $[A][C]$ but not $[C][A]$.  Given that, we also expect the output to be a $2 \times 3$ matrix. So, we have
    \[
    [A][C] = \begin{pmatrix} 4 & 3 & 10 & 2 \\ 1 & 1 & 0 & 9 \end{pmatrix}\begin{pmatrix} 0 & 0 & 9 \\ 7 & 9 & 9 \\ 1 & 9 & 9 \\ 3 & 3 & 1\end{pmatrix} = \begin{pmatrix} 37 & 123 & 155 \\ 34 & 36 & 27 \end{pmatrix}.
    \]

    \item $[B]$ is a $3 \times 3$ matrix so we can take $[C][B]$ but not $[B][C]$.  We get
    \[
        [C][B] = \begin{pmatrix} 0 & 0 & 9 \\ 7 & 9 & 9 \\ 1 & 9 & 9 \\ 3 & 3 & 1\end{pmatrix} \begin{pmatrix} 8 & 5 & 8 \\ 10 & 9 & 2 \\ 4 & 6 &3 \end{pmatrix} = \begin{pmatrix} 36 & 54 & 27 \\ 182 & 170 & 101 \\ 134 & 140 & 53 \\ 58 & 48 & 33 \end{pmatrix}.
    \]
    \item We can always add a matrix to itself, so, for example $[A]+[A]$, $[B]+[B]$, and $[C]+[C]$ make sense. However, since the dimensions of $[A]$, $[B]$, and $[C]$ all differ, we cannot add in any other way.
    \item The number of columns of a matrix denotes the input dimension $m$, and the number of rows denotes the output dimension $n$.  So
    \[
        A \colon \R^4 \to \R^2, \quad B \colon \R^3 \to \R^3, \quad C \colon \R^3 \to \R^4.
    \]
\end{enumerate}
\end{solution}

\newpage
\begin{problem}
Solve the following equation.
\[
\begin{pmatrix} 1 & 1 & 1 \\ 1 & 2 & 1 \\ 1 & 2 & 2 \end{pmatrix} \begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} 6 \\ 8 \\ 11 \end{pmatrix}.
\]
\end{problem}
\begin{solution}
First, we create the augmented matrix
\[
\left(\begin{tabular}{ccc|c} 1 & 1 & 1 & 6 \\ 1 & 2 & 1 & 8 \\ 1 & 2 & 2 & 11 \end{tabular} \right).
\]
We can subtract R1 from both R2 and R3 to get
\[
\left(\begin{tabular}{ccc|c} 1 & 1 & 1 & 6 \\ 0 & 1 & 0 & 2 \\ 0 & 1 & 1 & 5 \end{tabular} \right).
\]
Then subtract R3 from R1 to get
\[
\left(\begin{tabular}{ccc|c} 1 & 0 & 0 & 1 \\ 0 & 1 & 0 & 2 \\ 0 & 1 & 1 & 5 \end{tabular} \right).
\]
Finally, subtract R2 from R3 to get
\[
\left(\begin{tabular}{ccc|c} 1 & 0 & 0 & 1 \\ 0 & 1 & 0 & 2 \\ 0 & 0 & 1 & 3 \end{tabular} \right).
\]
This yields our result in the right most column in that $x=1$, $y=2$, and $z=3$.
\end{solution}

\newpage
\begin{problem}
Consider the space of polynomials of degree at most 3, $P_3(\C)$.
\begin{enumerate}[(a)]
    \item Using the basis
    \[
        B = \{ 1, x, x^2, x^3\},
    \]
    determine a matrix representation for the linear transformation $\frac{d}{dx} \colon P_3(\C) \to P_3(\C)$.
    \item Show that the set of Legendre polynomials
    \[
        B_L = \left\{f_0 = \sqrt{\frac{1}{2}}, ~ f_1 = \sqrt{\frac{3}{2}}x, ~ f_2 = \sqrt{\frac{5}{8}} (1-3x^2),~ f_3=\sqrt{\frac{63}{8}}\left(x-\frac{5x^3}{3}\right) \right\}
    \]
    is a basis for $P_3(\C)$.
    \item Using the basis $B_L$ instead, compute a matrix representation for the linear transformation $\frac{d}{dx}$.
\end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
\item Using the basis $B$, we can take an arbitrary degree at most 3 polynomial and write it as a column vector by
\[
a_0 + a_1 x + a_2x^2 + a_3x^3 = \begin{pmatrix} a_0 \\ a_1 \\ a_2 \\ a_3 \end{pmatrix}
\]
that is, we let
\[
1 = \begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \end{pmatrix},\quad x = \begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix},\quad x^2 = \begin{pmatrix} 0 \\ 0 \\ 1 \\ 0 \end{pmatrix},\quad x^3 = \begin{pmatrix} 0 \\ 0 \\ 0 \\ 1 \end{pmatrix}.
\]
Then we can compute
\[
\frac{d}{dx} (a_0 + a_1 x + a_2x^2 + a_3x^3) = a_1 + 2a_2 x + 3a_3 x^2 = \begin{pmatrix} a_1 \\ 2a_2 \\ 3a_3 \\ 0 \end{pmatrix}.
\]
To get a matrix for this expression, we have
\[
\left[ \frac{d}{dx}\right] \begin{pmatrix} a_0 \\ a_1 \\ a_2 \\ a_3 \end{pmatrix} = \begin{pmatrix} a_1 \\ 2a_2 \\ 3a_3 \\ 0 \end{pmatrix}
\]
which one can verify is
\[
\left[ \frac{d}{dx}\right] = \begin{pmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 2 & 0 \\ 0 & 0 & 0 & 3 \\ 0 & 0 & 0 & 0 \end{pmatrix}.
\]
\item Let $a_0 + a_1 x + a_2x^2 + a_3x^3$ be an arbitrary polynomial. To show $B_L$ is a basis, we need to show that using elements of $B_L$ we can generate this arbitrary polynomial by a linear combination in a unique way. So we must solve
\[
b_0 f_0 + b_1 f_1 + b_2 f_2 + b_3 f_3 = a_0 + a_1 x + a_2x^2 + a_3x^3
\]
for the $b_j$ terms. Writing this out more completely,
\begin{align*}
b_0 f_0 + b_1 f_1 + b_2 f_2 + b_3 f_3 &= b_0 \sqrt{\frac{1}{2}}+ b_1 \sqrt{\frac{3}{2}}x + b_2 \sqrt{\frac{5}{8}} (1-3x^2) + b_3 \sqrt{\frac{63}{8}}\left(x-\frac{5x^3}{3}\right)\\
&= \left(b_0 \sqrt{\frac{1}{2}} + b_2 \sqrt{\frac{5}{8}}\right) + \left(b_1 \sqrt{\frac{3}{2}} + b_3 \sqrt{\frac{63}{8}}\right)x +  b_2 3\sqrt{\frac{5}{8}} x^2 + b_3 \frac{5}{3}\sqrt{\frac{63}{8}} x^3.
\end{align*}
Now, looking at the $x^2$ and $x^3$ terms we can see that
\[
b_2 = \frac{1}{3}\sqrt{\frac{8}{5}} a_2 \qquad \textrm{and} \qquad b_3 = \frac{3}{5} \sqrt{\frac{8}{63}}a_3.
\]
It then follows that
\[
b_0 = \sqrt{2}\left(a_0  -\frac{1}{3}a_2\right) \qquad \textrm{and} \qquad b_1 = \sqrt{\frac{2}{3}} \left( a_1 - \frac{3}{5}a_3 \right).
\]
This shows we have found $b_j$ uniquely so $B_L$ is indeed a basis.
\item Now, if we take this new basis $B_L$ then we have
\[
b_0 f_0 + b_1 f_1 + b_2 f_2 + b_3 f_3 = \begin{pmatrix} b_0 \\ b_1 \\ b_2 \\ b_3 \end{pmatrix}
\]
where again
\[
f_0 = \begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \end{pmatrix},\quad f_1 = \begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix},\quad f_2 = \begin{pmatrix} 0 \\ 0 \\ 1 \\ 0 \end{pmatrix},\quad f_3 = \begin{pmatrix} 0 \\ 0 \\ 0 \\ 1 \end{pmatrix}.
\]
This time, let us compute the transformation of each individual basis element
\begin{align*}
\frac{d}{dx}f_0 &= 0\\
\frac{d}{dx}f_1 &= \sqrt{\frac{3}{2}}\\
\frac{d}{dx}f_2 &= -6\sqrt{\frac{5}{8}}x\\
\frac{d}{dx}f_3 &= \sqrt{\frac{63}{8}} \left( 1 - 5 x^2 \right).
\end{align*}
Then our goal is to write each one of these answers in terms of the basis $B_L$ as well. That is,
\begin{align*}
\frac{d}{dx}f_0 &= 0 = 0f_0 + 0 f_1 + 0 f_2 + 0 f_3\\
\frac{d}{dx}f_1 &= \sqrt{\frac{3}{2}} = \sqrt{\frac{1}{3}} f_0 + 0 f_1 + 0 f_2 + 0 f_3 \\
\frac{d}{dx}f_2 &= -6\sqrt{\frac{5}{8}}x = 0f_0 + \sqrt{15}f_1 + 0 f_2 + 0 f_3 \\
\frac{d}{dx}f_3 &= \sqrt{\frac{63}{8}} \left( 1 - 5 x^2 \right) = \frac{1}{2} \sqrt{14} f_0 + 0 f_1 + \sqrt{35} f_2 + 0 f_3.
\end{align*}
Hence, we use these as columns for the matrix
\[
\left[ \frac{d}{dx}\right]_{B_L} = \begin{pmatrix} 0 & \sqrt{\frac{1}{3}} & 0 & \frac{1}{2}\sqrt{14}\\ 0 & 0 & \sqrt{15} & 0 \\ 0 & 0 & 0 &  \sqrt{35} \\ 0 & 0  & 0 & 0 \end{pmatrix}.
\]
\end{enumerate}
\end{solution}

\begin{problem}
Let $C^\omega(\C)$ be the set of analytic functions (functions that have a power series representation), i.e., functions of the form
\[
f(x) = \sum_{n=0}^\infty a_n x^n,
\]
where $a_n\in \C$ for $n=0,1,2,\dots$. Let us compare this with the vector space of polynomials $P_N(\C)$.
\begin{enumerate}[(a)]
    \item Argue that $C^\omega(\C)$ is a vector space. \emph{Hint: show what addition and scalar multiplication look like.}
    \item Show that the space of polynomials of degree at most $N$, $P_N(\C)$ is a subspace of $C^\omega(\C)$.
    \item Let $f,g\in C^\omega(\C)$ be given by
    \[
        f(x)=\sum_{n=0}^\infty a_n x^n \qquad \textrm{and} \qquad g(x) = \sum_{n=0}^\infty b_n x^n,
    \]
    then define an inner product on $C^\omega(\C)$ by taking
    \[
    \langle f,g\rangle \coloneqq \sum_{n=0}^\infty a_n b_n^*.
    \]
    Now, write $h(x)$ as a Taylor series centered at $x=0$ and show that
    \[
    h^{(k)}(0) = k!\langle h,x^k \rangle.
    \]
    \item Show that the $N^\textrm{th}$ order Taylor approximation for the function $h(x)$ centered at $x=0$ is the projection onto the subspace spanned by the functions
    \[
    S=\{1,x,x^2,\dots,x^N\}.
    \]
    This projection is given by
    \[
    \textrm{proj}_S(h) = \sum_{n=0}^N \langle h, x^n \rangle x^n.
    \]
\end{enumerate}
\begin{solution}~
\begin{enumerate}[(a)]
    \item For $C^\omega(\C)$ to be a vector space, we need the properties of addition and scalar multiplication to hold and we need to identify the zero vector and identity element of the field. First, note that $0\in C^\omega(\C)$ and let $f,g \in C^\omega(\C)$ be given by
    \[
        f(x) = \sum_{n=0}^\infty a_n x^n \qquad \textrm{and} \qquad g(x) = \sum_{n=0}^\infty b_n x^n.
    \]
    It is clear that $0+f= f$ so the function $0$ acts as the zero vector. Similarly, $1\in \C$ satisfies $1f = f$. Then we can note that addition of functions is associative and
    \[
    (\alpha + \beta)f = (\alpha + \beta) \sum_{n=0}^\infty a_n x^n = \alpha \sum_{n=0}^\infty  a_n x^n + \beta\sum_{n=0}^\infty  a_n x^n = \alpha f + \beta f
    \]
    likewise
    \[
    \alpha(f+g) = \alpha \left(\sum_{n=0}^\infty a_n x^n + \sum_{n=0}^\infty b_n x^n\right) = \alpha \sum_{n=0}^\infty a_n x^n + \alpha \sum_{n=0}^\infty b_n x^n.
    \]
    In fact, the key insight is that a linear combination
    \[
    \alpha f+ \beta g = \sum_{n=0}^\infty (\alpha a_n + \beta b_n)x^n
    \]
    so we are merely adding linear combinations of the sequences $a_n$ and $b_n$ together. If you'd like, you could use the sequence where $1$ is in the $j$th entry of a sequence as a basis and write
    \[
    f = \begin{pmatrix} a_0 \\ a_1 \\ a_2 \\ a_3 \\ \vdots \end{pmatrix} \qquad \textrm{and} \qquad g = \begin{pmatrix} b_0 \\ b_1 \\ b_2 \\ b_3 \\ \vdots \end{pmatrix}.
    \]
    In this sense, the elements in $C^\omega(\C)$ are nothing but sequences or infinitely tall column vectors. However, they are special sequences that converge quickly enough to zero!
    \item We have already argued that $P_N(\C)$ is a vector space itself and to see that it is a subspace, just note that any polynomial function is automatically a power series. A polynomial of degree at most $N$ is just a power series where all the coefficients where $k>N$ must $a_{k}=0$. That is
    \[
    p(x) = \sum_{n=0}^N c_n x^n
    \]
    is a polynomial of degree at most $N$ and if we take $c_k=0$ for $k>N$, then
    \[
    \sum_{n=0}^\infty c_n x^n
    \]
    shows that the polynomials are a subspace. Another way to see this is,
    \[
    p = \begin{pmatrix} c_0 \\ c_1 \\ c_2 \\ \vdots \\ c_N \\ 0 \\ 0 \\ \vdots \end{pmatrix}.
    \]
    \item Based on the previous part, the function $x^k$ is given by a power series
    \[
    x^k = \sum_{n=0}^\infty \alpha_n x^n
    \]
    where $\alpha_k =1$ but $\alpha_j=0$ for all $j\neq k$. Then, for $h(x) = \sum_{n=0}^\infty \frac{h^{(n)}(0)}{n!}x^n$ we have
    \begin{align*}
        \langle h, x^k \rangle &= \sum_{n=0}^\infty \frac{h^{(n)}(0)}{n!}\alpha_n^*.
    \end{align*}
    But the only term in the series that survives is when $n=k$ since $\alpha_j=0$ for all $j\neq k$ therefore
    \[
    \langle h, x^k \rangle = \frac{h^{(k)}(0)}{n!}\alpha_n
    \]
    Hence,
    \[
h^{(k)}(0) = k!\langle h,x^k \rangle.
    \]
\item Using the logic from the previous part,
\[
\textrm{proj}_S(h) = \sum_{n=0}^N \langle h, x^n \rangle  x^n = \sum_{n=0}^N \frac{h^{(k)}(0)}{n!}x^n.
\]

\end{enumerate}
\end{solution}
\end{problem}








\end{document}