%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document data
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt]{article} %report allows for chapters
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{preamble}

\newcommand{\vhat}{\boldsymbol{\hat{v}}}
\newcommand{\ehat}{\boldsymbol{\hat{e}}}
\newcommand{\Span}{\operatorname{Span}}
\newcommand{\image}{\operatorname{im}}
\newcommand{\bvec}[1]{\boldsymbol{\vec{#1}}}

\begin{document}

\begin{center}
   \textsc{\large MATH 271, Homework 9, \emph{Solutions}}\\
   \textsc{Due November 15$^\textrm{th}$}
\end{center}
\vspace{.5cm}



\begin{problem}
Compute the following:
\begin{enumerate}[(a)]
    \item
    \[
    [A]=\begin{pmatrix} 1& 1& 1 \end{pmatrix}
    \begin{pmatrix} 2\\ 1\\ 3 \end{pmatrix}.
    \]
    \item
    \[
    [B]=\begin{pmatrix} 1& 2& 3& 4\\ 5& 6& 7& 8\\ 9& 10& 11& 12\end{pmatrix}
    \begin{pmatrix} 3& 2\\ 2& 3\\ 3& 2\\ 2& 3\end{pmatrix}
    \]
    \item Take
    \[
    [M]=\begin{pmatrix} 10& 15\\ 20& 10 \end{pmatrix}
    \]
    and
    \[
    [N]=\begin{pmatrix} 1 & 2\\ 2& 1\end{pmatrix}.
    \]
    Compute $[M][N]$ and $[N][M]$ to see that matrices do not commute in general.
\end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item Since we have a $1\times 3$-matrix multiplied with a $3\times 1$-matrix, we know that $[A]$ should be a $1\times 1$-matrix.
    \begin{align*}
    [A]&=\begin{pmatrix} 1 & 1 & 1 \end{pmatrix} \begin{pmatrix} 2 \\ 1 \\ 3 \end{pmatrix}\\
    &= (1\cdot 2 + 1 \cdot 1 + 1 \cdot 3)\\
    &= (6).
    \end{align*}
    \item Here, we should expect that $[B]$ is a $3\times 2$-matrix.
    \[
    [B] = \begin{pmatrix} 24 & 26 \\ 64 & 66 \\ 104 & 106 \end{pmatrix}.
    \]
    \item Here $[M]$ and $[N]$ are square, so multiplying will give us the same shape matrix.  We have
    \[
    [M][N] = \begin{pmatrix} 40 & 35 \\ 40 & 50 \end{pmatrix},
    \]
    as well as
    \[
    [N][M] = \begin{pmatrix} 50 & 35 \\ 40 & 40 \end{pmatrix}.
    \]
    From this we can see that $[M][N]\neq [N][M]$ in general!
\end{enumerate}
\end{solution}

\newpage
\begin{problem}
A linear transformation $T\colon \R^3 \to \R^3$ is given by the matrix
\[
[T]= \begin{pmatrix}
1& 2& 0\\
2& 1& 2\\
0& 2& 1
\end{pmatrix}.
\]
\begin{enumerate}[(a)]
    \item Compute how $T$ transforms the standard basis elements for $\R^3$. That is, find
    \[
    T(\ehat_1), \qquad
    T(\ehat_2), \qquad
    T(\ehat_3)
    \]
    and relate these values to the columns of $[T]$.
    \item Is the transformed basis $T(\ehat_1)$, $T(\ehat_2)$, and $T(\ehat_3)$ linearly independent? Do these vectors form a basis for $\R^3$?
    \item If we apply this linear transformation to the unit cube (that is, all points who have $(x,y,z)$ coordinates with $0\leq x \leq 1$, $0\leq y \leq 1$, and $0\leq z \leq 1$), what will the volume of the transformed cube be? (\emph{Hint: use the determinant.})
\end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item The point here is that we can understand the matrix $[T]$ and matrix multiplication better by seeing how the basis vectors are transformed. So we have
    \begin{align*}
        T(\ehat_1) &= \begin{pmatrix}
1& 2& 0\\
2& 1& 2\\
0& 2& 1
\end{pmatrix} \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}\\
&= \begin{pmatrix} 1 \\ 2 \\ 0 \end{pmatrix}\\
&= \ehat_1 + 2 \ehat_2,
    \end{align*}
    which is just the first column of the matrix. Then we have
    \begin{align*}
                T(\ehat_2) &= \begin{pmatrix}
1& 2& 0\\
2& 1& 2\\
0& 2& 1
\end{pmatrix} \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}\\
&= \begin{pmatrix} 2 \\ 1 \\ 2 \end{pmatrix}\\
&= 2\ehat_1 + \ehat_2 + 2\ehat_3,
    \end{align*}
    which is just the second column of the matrix. Lastly we have
    \begin{align*}
                T(\ehat_3) &= \begin{pmatrix}
1& 2& 0\\
2& 1& 2\\
0& 2& 1
\end{pmatrix} \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}\\
&= \begin{pmatrix} 0 \\ 2 \\ 1 \end{pmatrix}\\
&= 2\ehat_2 + \ehat_3,
    \end{align*}
    which is the last column of the matrix.
    \item Yes. You can see this in the following ways: By row reducing to the identity matrix, by showing that the kernel is trivial, or by computing the determinant and showing it is nonzero. Let us use the determinant.
\[
    \det [T] = -7.
\]
therefore the columns are linearly independent. Since the columns are exactly $T(\ehat_j)$ by definition, those vectors are independent. Since it is a set of 3 independent vectors in $\R^3$, it is a basis and all of $\R^3$ is in the span of those transformed vectors.
    \item The three basis vectors
    \[
    \ehat_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \qquad \ehat_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}, \qquad \ehat_3 =\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}
    \]
    define the volume of the unit cube. That is, the parallelepiped generated by $\ehat_1$, $\ehat_2$, and $\ehat_3$ is the unit cube. Hence, if we know how these vectors are transformed, we just need to find the volume of the paralellepiped given by the transformed vectors $T(\ehat_1)$, $T(\ehat_2)$, and $T(\ehat_3)$.  Now, we can collect these vectors into a matrix,
    \begin{align*}
    \begin{pmatrix} \vert & \vert & \vert \\ T(\ehat_1) & T(\ehat_2) & T(\ehat_3) \\ \vert & \vert & \vert \end{pmatrix} &= \begin{pmatrix}  1& 2& 0\\
2& 1& 2\\
0& 2& 1\end{pmatrix},
    \end{align*}
    which is exactly $[T]$ and this should not be shocking since this is how we defined a matrix representation in the first place. Now, the determinant of the matrix gives us the signed volume of the parallelepiped generated by the three column vectors, and hence
    \[
    \mathrm{Area}=|\det[T]|=|-7|=7.
    \]
\end{enumerate}
\end{solution}

\newpage
\begin{problem}~
\begin{enumerate}[(a)]
    \item Show that for any $2\times 2$-matrix that the sign of the determinant changes if either a row or column is swapped. \emph{Note: this is true for square matrices of any size}.
    \item Show that for any $2\times 2$-matrix that multiplying a column by a constant scales the determinant by that constant as well. \emph{Note: this is true for square matrices of any size.}
    \item Show that for any $2\times 2$-matrix that adding a scalar multiple one column to the other will not change the determinant. \emph{Note: this is true in broader generality. In fact, adding linear combinations of columns to another column will not change the determinant.}
    \item Using these facts, argue that a square matrix with columns that are linearly dependent must have a determinant of zero.
\end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item Let
    \[
    [A]=\begin{pmatrix} a & b \\ c & d \end{pmatrix}
    \]
    be an arbitrary $2\times 2$-matrix.  Then we have
    \[
    \det([A])=ad-bc.
    \]
    Now, if we swap rows we have
    \[
    \left| \begin{matrix} c & d \\ a & b \end{matrix} \right| = bc-ad = -(ad-bc).
    \]
    Now, we can do the same with columns to get
    \[
    \left| \begin{matrix} b & a \\ d & c \end{matrix} \right| = bc-ad = -(ad-bc).
    \]
    \item Let us compute the determinant of
    \[
    \left| \begin{matrix} \alpha a & b \\ \alpha c & d \end{matrix} \right| = \alpha ad - \alpha bc = \alpha(ad-bc).
    \]
    Similarly, we will have the same if we scale the other column. In fact, this is true for rows as well.
    \item Let us add the first column to the second. We get
    \[
     \left| \begin{matrix} \alpha a & \alpha a+b \\ \alpha c & \alpha c+d \end{matrix} \right| = a(\alpha c+d)-(\alpha a+b)c = \alpha ac+ad-\alpha ac-bc = ad-bc.
    \]
    The same will be true if we add a scalar copy of column 2 to column 1.
    \item Let us just show this for a $3\times 3$-matrix as the argument is the same for the most general case.  Let
\[
[A] = \begin{pmatrix} \vert & \vert & \vert \\ \colA_1 & \colA_2 & \colA_3 \\ \vert & \vert & \vert \end{pmatrix}.
\]
Then if the columns of $[A]$ are linearly dependent, we have that
\[
\alpha_1 \colA_1 + \alpha_2 \colA_2 + \alpha_3 \colA_3 = \zerovec
\]
with at least one $\alpha_i\neq 0$.  Specifically, this means that one vector can be written as a linear combination of the others. That is we can take
\[
\colA_3 = \frac{-1}{\alpha_3} (\alpha_1 \colA_1 + \alpha_2 \colA_2),
\]
so long as $\alpha_3\neq 0$.  If $\alpha_3 = 0$, then choose another vector to write as a linear combination of the others. Then, we can subtract the quantity
\[
\frac{-1}{\alpha_3} (\alpha_1 \colA_1 + \alpha_2 \colA_2)
\]
from column 3 in $[A]$ to get
\[
\begin{pmatrix} \vert & \vert & \vert \\ \colA_1 & \colA_2 & \zerovec \\ \vert & \vert & \vert \end{pmatrix},
\]
which has a determinant of zero. Since we only added a linear combination of columns to another column, this did not change the determinant and hence we must have $\det([A])=0$. I will leave it open for you to do this for an $n\times n$-matrix.
\end{enumerate}
\end{solution}

\newpage
\begin{problem}
Consider the equation
\[
[A]\vecv = \zerovec,
\]
where
\[
[A] = \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 0 \end{pmatrix}.
\]
\begin{enumerate}[(a)]
    \item Are the columns of $[A]$ linearly independent or dependent? Explain.
    \item What vector(s) $\vecv$ satisfy this equation? In other words, what is $\Null([A])$?
    \item Using what you found above, what must $\det([A])$ be equal to? \emph{Hint: you do not need to compute the determinant!}
\end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item The columns are dependent as the leftmost column is equal to the rightmost column.
    \item To solve the homogeneous equation we take
    \begin{align*}
        [M]=\left(\begin{array}{ccc|c}
            0 & 1 & 0 &0 \\
            1 & 0 & 1 &0\\
            0 & 1 & 0 &0
        \end{array}\right).
    \end{align*}
    Then we can subtract row one from row three to get
    \[
    \left(\begin{array}{ccc|c}
            0 & 1 & 0 &0\\
            1 & 0 & 1 &0\\
            0 & 0 & 0 &0
        \end{array}\right)
    \]
    which corresponds to the equations
    \begin{align*}
        0x+y+0z&=0\\
        x+0y+z&=0\\
        0x+0y+0z&=0.
    \end{align*}
    Hence we have that $z=-x$ and $y=0$.  Thus any vector of the form
    \[
    \vecv = \begin{pmatrix} t \\ 0 \\ -t \end{pmatrix}
    \]
    for any $t\in \R$ is a solution to this equation. In other words, the set described above is $\ker[A]$.
    \item The determinant must be equal to zero since $\ker[A]$ is nontrivial (i.e., it contains more than just the zero vector). One can also note the columns are dependent which implies this as well.  This goes to show a bit on how these ideas are all connected.
\end{enumerate}
\end{solution}

\newpage
\begin{problem}
Compute the following.
\begin{enumerate}[(a)]
    \item
    \[
    \det[A]=\left| \begin{array}{ccc}
    -3& 1 & 5\\
    -3& 4 & 2\\
    -3& 2 & 1
    \end{array}\right|
    \]
    \item
    \[
    \det[B]=\left| \begin{array}{ccc}
    1& 2& 3\\
    4& 5& 6\\
    7& 8& 9
    \end{array}\right|
    \]
    \item Compute $\det([A][B])$ using properties of the determinant. \emph{Hint: this should be very quick to do. Do not compute the product of the matrices $[A]$ and $[B]$!}
    \item Compute $\tr([C])$ and $\tr([D])$ where
    \[
[C]=\begin{pmatrix} 1 & 0 & 2 \\ 2  & 1 & 3 \\ -2 & -2 & 0 \end{pmatrix} \qquad \textrm{and} \qquad [D]=\begin{pmatrix} -3 & 1 & 1 \\ 2 & -2 & 4 \\ -1 & -1 & -1 \end{pmatrix}.
\]
    \item Compute $\tr([C][D])$ and compare it to $\tr([D][C])$.
\end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item We can expand along any row or column and in this case, there are no zeros to make the computation quicker. So we have
    \begin{align*}
        \left| \begin{matrix} -3 & 1 & 5 \\ -3 & 4 & 2 \\ -3 & 2 & 1 \end{matrix} \right| &= -3 \left| \begin{matrix} 4 & 2 \\ 2 & 1 \end{matrix} \right| -1 \left| \begin{matrix} -3 & 2 \\ -3 & 1 \end{matrix} \right| +5 \left| \begin{matrix} -3 & 4 \\ -3 & 2 \end{matrix} \right|\\
        &= -3(4-4)-1(-3+6)+5(-6+12)\\
        &= 27.
    \end{align*}
    \item Similarly, we get
    \[
    \det([B])= 0.
    \]
    \item We know that $\det([A][B])=\det([A])\det([B])$ and thus we have that $\det([A][B])=0.$
    \item The trace is the sum of the diagonal entries. Thus we have
    \begin{align*}
        \tr[C]&=1+1+0=2,\\
        \tr[D]&=-3-2-1=-6.
    \end{align*}
    \item Then we can compute $[C][D]$,
    \[
    [C][D]=\begin{pmatrix}
    -5 & -1 & -1 \\ -7 & -3 & 3 \\ 2 & 2 & -10
    \end{pmatrix}.
    \]
    Hence we have
    \[
    \tr([C][D]) = -18.
    \]
    Note that under cyclic permutations, the trace is invariant, hence
    \[
    \tr([C][D])=\tr([D][C])
    \]
    even though $[C][D]\neq [D][C]$

\end{enumerate}
\end{solution}


\newpage
\begin{problem}
Consider some linear transformation $T\colon \R^n \to \R^m$.  Let $\vecv_1, \dots, \vecv_k$ be vectors in $\operatorname{Null}(T)$.
\begin{enumerate}[(a)]
    \item Show that the span of these vectors is also in the kernel of $T$.
    \item How many linearly independent vectors can be in the kernel? Give bounds using $n$ and $m$.
\end{enumerate}
\end{problem}
\begin{solution} ~
\begin{enumerate}[(a)]
    \item An arbitrary vector $\vecv$ in the span of $\vecv_1,\dots,\vecv_k$ is given by
    \[
    \vecv= \alpha_1 \vecv_1 + \alpha_2 \vecv_2 + \cdots + \alpha_k \vecv_k.
    \]
    Since $\vecv$ is arbitrary, if we show $\vecv \in \ker T$, then we are done.  So we take
    \begin{align*}
        T(\vecv) &= T(\alpha_1 \vecv_1 + \alpha_2 \vecv_2 + \cdots + \alpha_k \vecv_k )\\
        &= \alpha_1 T(\vecv_1) + \alpha_2 T(\vecv_2) + \cdots + \alpha_k T(\vecv_k) && \textrm{by linearity of $T$}\\
        &= 0 &&\textrm{since $T(\vecv_i)=0$ for all $i=1,\dots, k$.}
    \end{align*}
    Thus the span of $\vecv_1,\dots,\vecv_k$ is also in the nullspace of $T$.
    \item With $T\colon \R^n \to \R^m$ we can take the transformation $T(\vecv)=\zerovec$ for every vector $\vecv \in \R^n$.  Note that this transformation always exists and will always have the largest kernel.  Thus, the kernel of $T$ would have as many as $n$-linearly independent vectors since there can be at most $n$-linearly independent vectors in $\R^n$. This fact is independent of the value for $m$.

    Taking $m$ into account now, if $m<n$, then we must have $n-m$ vectors in the kernel of $T$ at the very least. The argument is somewhat geometrical as $T$ removes $n-m$ dimensions in the process and as such, we remove $n-m$ linearly independent vectors (as those vectors span those removed dimensions).  Thus, if $m<n$ we have that
    \[
        n-m \leq \textrm{number of L.I. vectors in kernel of $T$} \leq n.
    \]
    In the case $m\geq n$ we have
    \[
       0 \leq \textrm{number of L.I. vectors in kernel of $T$} \leq n.
    \]
    To see why this is true, we let $\vecv = v_1 \ehat_1 + v_2 \ehat_2 + \cdots + v_n \ehat_n$ and note that if $m\geq n$ we can take
    \[
    T(\vecv) = v_1 \ehat_1 + v_2 \ehat_2 + \cdots + v_n \ehat_n + 0 \ehat_{n+1} +\cdots + 0 \ehat_{m},
    \]
    which shows that there are no nontrivial vectors in the kernel of this $T$.

\end{enumerate}

\end{solution}

\newpage
\begin{problem}~
    Suppose that the operator $T\colon V \to V$ has a nonzero kernel (e.g., some vector $\vecv$ other than $\zerovec$ is in the kernel). Prove that $T$ has no inverse. \emph{Hint: this means you can construct a vector that is not in the image of $T$!}
\end{problem}
\begin{solution}
I will give two proofs for this:
\begin{itemize}
\item It is true that
\[
\dim V = \dim \ker T + \dim \image T.
\]
Therefore if $\dim \ker T >0$, then $\dim \image T < \dim V$. Hence, there exists at least one nonzero vector $\vecv \in V$ that is not in the image of $T$. Therefore, the is no $\vecu \in V$ such that $T(\vecu)=\vecv$ and so $T$ has no inverse.
\item Since $T$ has a nonzero kernel, take $\vecv \neq \zerovec \in \ker T$. Then $T(\vecv) = \zerovec = T(\zerovec)$ Therefore if $T^{-1}$ did exist, it must be that $T^{-1}(\zerovec) = \vecv$ and $T^{-1}(\zerovec) = \zerovec$. This is a contradiction so the supposition must be false.
\end{itemize}
\end{solution}

\newpage
\begin{problem}
The previous problem will be very helpful for these two parts.
\begin{enumerate}[(a)]
    \item Let $T\colon V\to V$ be an operator such that $\det [T]=0$. Explain why there exists a solution to the homogeneous equation $T\vecu = \zerovec.$
    \item Suppose $S\colon V \to V$ is another operator such that $\det [S] \neq 0$. Explain why there exists a solution to the inhomogeneous equation $S\vecv = \vecw$ for any $\vecw \in V$.
\end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item Since $\det [T] = 0$ it must be that the kernel of $T$ is nonzero. You can see this fact in many ways. For instance, the columns of $[T]$ are linearly dependent and hence you can take a linear combination of $T(\evec_j)$ and get the zero vector, for instance
    \[
    u_1 T(\evec_1) + \cdots + u_n T(\evec_n) = \zerovec
    \]
    where not all $u_j$ are zero. Hence, there exists a vector $\vecu = \sum_{j=1}^n u_j \evec_j \in \ker T$ and by definition/construction $T \vecu = \zerovec$.
    \item Since $\det[S]\neq 0$ then the columns of $S$ are linearly independent. Since there are $n$-linearly independent vectors in an $n$-dimensional space (assuming $V$ is dimension $n$), they form a basis and span $V$. Note that for $\vecv=\sum_{j=1}^n v_j \ehat_j$ and
\[
S \vecv = \sum_{j=1}^n v_j S(\ehat_j)
\]
which is just a linear combination of the columns of $[S]$. Since the columns of $[S]$ are a basis, for any vector $\vecw \in V$, we have $\vecw \in \Span \{S(\ehat_1),\dots, S(\ehat_n)\}$ which is exactly what $S\vecv$ dictates.

\end{enumerate}
\end{solution}

\newpage
\begin{problem}
Prove that the eigenvectors with eigenvalue 0 of an operator $T\colon V \to V$ correspond to vectors in the kernel of $T$.
\end{problem}
\begin{solution}
Let $\vecv \in V$ be an eigenvector with eigenvalue $\lambda = 0$. Then
\[
T \vecv = \lambda \vecv = 0 \vecv = 0.
\]
Thus $\vecv \in \ker T$.
\end{solution}

\newpage
\begin{problem}
Consider the linear operator $J\colon \R^2 \to \R^2$ defined by
\[
J(\ehat_1) = \ehat_2 \qquad \textrm{and} \qquad J(\ehat_2) = -\ehat_1.
\]
\begin{enumerate}[(a)]
    \item Show that operator polynomial
    \[
    P(J) \coloneqq J^2 + I \colon \R^2 \to \R^2
    \]
    annihilates $\R^2$. Or, said another way, show that every $\vecv \in \R^2$ is in the kernel of $P(J)$.
    \item Show that the characteristic polynomial of $J$ is
    \[
    p(\lambda) = \lambda^2 + 1.
    \]
    Does this coincide with $P(J)$? \emph{If need be, use your matrix representation $[J]$ from the previous homework.}
    \item Compute the eigenvalues $\lambda_1$ and $\lambda_2$ of $J$.
    \item Compute the corresponding eigenvectors of $J$.
    \item If we don't allow for complex scalars, $J$ has no eigenvalues. However, $J^2$ does have only real eigenvalues. Using (a), show that $J$ has eigenvalue $\lambda=-1$ with eigenvectors $\ehat_1$ and $\ehat_2$.
    \item (Bonus) Can you argue that any nonzero rotation of $\R^2$ must have imaginary eigenvalues?
\end{enumerate}
\end{problem}
\begin{solution}
\begin{enumerate}[(a)]
\item Let $\vecv \in \R^2$ be given by $\vecv = v_1 \ehat_1 + v_2 \ehat_2$. Then
\begin{align*}
(J^2+I)\vecv &= J^2 \vecv + I \vecv\\
& = J (J(v_1 \ehat_1 + v_2 \ehat_2)) \\
&= J(-v_2 \ehat_1 + v_1 \ehat_2) + \vecv\\
&= -v_1 \ehat_1 - v_2 \ehat_ 2 + \vecv\\
&= \zerovec.
\end{align*}
\item We can form a matrix
\[
[J] = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}.
\]
Then
\[
\det([J]- \lambda [I]) = \det \begin{pmatrix} -\lambda & -1 \\ 1 & - \lambda \end{pmatrix} = \lambda^2 + 1.
\]
\item The eigenvalues are the roots to the characteristic polynomial so
\[
\lambda^2 + 1 = 0.
\]
The roots are $\lambda_1 = i$ and $\lambda_2 = -i$.
\item For $\lambda_1=i$, we take $([J]-i[I])\bvec{p}_1 = \zerovec$ where $\bvec{p}_1$ is the first eigenvector
\begin{align*}
([J]-i[I])\bvec{p}_1  =\begin{pmatrix} -i & -1 \\ 1 & -i \end{pmatrix} \begin{pmatrix} p_{11} \\ p_{21} \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}.
\end{align*}
Thus
\begin{align*}
    -i p_{11} - p_{21} &= 0\\
    p_{11} - ip_{21} &= 0.
\end{align*}
Multiplying the bottom equation by $i$ yields
\begin{align*}
    i p_{11} + p_{21} &= 0
\end{align*}
which can be added to the first equation to cancel it off. Hence
\[
p_{11} = ip_{21}.
\]
So just choose $p_{21}=1$ and
\[
\bvec{p}_1 = \begin{pmatrix} i \\ 1 \end{pmatrix}
\]

Similar work for $\lambda_2$ shows that a corresponding eigenvector is $\bvec{p}_2=\begin{pmatrix} -i \\ 1 \end{pmatrix}$.

\begin{remark}
The matrix $[P] = [\bvec{p}_1 ~ \bvec{p}_2]$ diagonalizes $[J]$.
\end{remark}
\item Just take
\[
J^2 (\ehat_1) = J(J(\ehat_1)) = J(\ehat_2)= -\ehat_1
\]
and
\[
J^2 (\ehat_2) = J(J(\ehat_2)) = J(-\ehat_1)= -J(\ehat_1) = -\ehat_2
\]
which shows both are eigenvectors with eigenvalue $1$.
\item An arbitrary rotation of some plane is given by the matrix
\[
\begin{pmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{pmatrix}
\]
for some choice of $\theta$ then the characteristic polynomial is
\[
\lambda^2 - 2 \lambda \cos \theta + 1
\]
which has roots
\begin{align*}
\lambda_1 &= \cos \theta - i \sin \theta \\
\lambda_2 &= \cos \theta + i \sin \theta.
\end{align*}
and the same eigenvectors as $[J]$.
\end{enumerate}
\end{solution}

\newpage

\begin{problem} For this problem, we will consider eigenvectors of three operators that act on the space of analytic functions $C^\omega(\C)$. Your goal should be to realize that these correspond to differential equations you have seen before.
\begin{enumerate}[(a)]
    \item Take the operator $\frac{d}{dx}\colon C^\omega(\C)\to C^\omega(\C)$. Show that the exponential $e^{kx}\in C^\omega(\C)$ is an eigenvector (or eigenfunction) with eigenvalue $k$. Write down the corresponding ODE. \emph{Hint: just by doing the problem properly, you will probably write down the ODE.}
    \item Take the operator $\frac{d^2}{dx^2}\colon C^\omega(\C)\to C^\omega(\C)$. Show that there are two eigenfunctions $e^{i\omega x}$ and $e^{-i\omega x}$ with eigenvalue $-\omega^2$. Write down the corresponding ODE. \emph{Hint: just by doing the problem properly, you will probably write down the ODE.}
    \item Take the operator $x\frac{d}{dx} \colon C^\omega(\C) \to C^\omega(\C)$. Find the eigenfunctions to this operator using the fact that this corresponds to a separable ODE.
\end{enumerate}
\end{problem}~
\begin{solution}~
\begin{enumerate}[(a)]
\item An eigenfunction of $\frac{d}{dx}$ is a function $f$ such that
\[
\frac{d}{dx} f = k f.
\]
So, take $f=e^{kx}$ then
\[
\frac{d}{dx} e^{kx} = k e^{kx}
\]
is an eigenfunction.
\item An eigenfunction of $\frac{d^2}{dx^2}$ is a function $f$ such that
\[
\frac{d^2}{dx^2} f = \lambda f = -\omega^2 f
\]
where I'm taking the liberty of using $-\omega^2$ as an eigenvalue since I already know the work here. Take $f_\pm=e^{\pm i\omega x}$ then
\[
\frac{d}{dx} e^{\pm i\omega x} = -\omega^2 e^{\pm i \omega x}
\]
is an eigenfunction.
\item An eigenfunction of $\frac{d}{dx}$ is a function $f$ such that
\[
x\frac{d}{dx} f = \lambda f.
\]
Then
\begin{align*}
    x\frac{d}{dx} f &= \lambda f\\
\iff \quad \frac{1}{f}\frac{df}{dx}  &= \lambda \frac{1}{x}\\
\iff \int \frac{1}{f} df &= \lambda \int \frac{1}{x} dx\\
\iff \ln f & = \lambda \ln x + c\\
\iff f &= c x^\lambda.
\end{align*}
Hence the eigenfunctions are $x^\lambda$.
\begin{remark}
On polynomials, the basis functions $x^j$ for $j=0,\dots, n$ are eigenvectors with eigenvalue $j$. So, in matrix notation for example take $P_3 (\C)$,
\[
\left[ x \frac{d}{dx} \right] = \begin{pmatrix} 1 & 0 & 0 & 0  \\ 0 & 2 & 0 & 0 \\ 0 & 0 & 3 & 0  \\ 0 & 0 & 0 & 4 \end{pmatrix}.
\]
If you'd like, you can see that $x$ acts on $P_3(\C)$ as a \emph{right shift operator}.
\end{remark}
\end{enumerate}
\end{solution}

\newpage
\begin{problem} Consider the Legendre polynomials
    \[
        B_L = \left\{f_0 = \sqrt{\frac{1}{2}}, ~ f_1 = \sqrt{\frac{3}{2}}x, ~ f_2 = \sqrt{\frac{5}{8}} (1-3x^2),~ f_3=\sqrt{\frac{63}{8}}\left(x-\frac{5x^3}{3}\right) \right\}
    \]
which form a basis for $P_3(\C)$.
\begin{enumerate}[(a)]
    \item For polynomials $f,g\in P_3(\C)$, define an inner product
    \[
\langle g,h\rangle \coloneqq \int_{-1}^1 gh^* dx.
\]
    Show (or find in the text or previous homeworks) evidence that the basis $B_L$ is orthonormal with respect to this inner product.
    \item Consider the operator
    \[
    \mathcal{L} \coloneqq (1-x^2)\frac{d^2}{dx^2} - 2x \frac{d}{dx} \colon P_3(\C) \to P_3(\C).
    \]
    Show that $\mathcal{L}$ is linear.
    \item Show that each Legendre polynomial $f_i$ is an eigenvector (or eigenfunction) of $\mathcal{L}$. What are the corresponding eigenvalues? How do these eigenvalues correspond to the $m$ that appears in Legendre's equation (see the section in our text).
\end{enumerate}
\end{problem}
\begin{solution}~
\begin{enumerate}[(a)]
    \item See Homework 6 Problem 4 solution.
    \item First we know that $\frac{d}{dx}$ is linear by previous homework. The composition of linear transformations are also linear transformations so $\frac{d^2}{dx^2}$ is linear. Next, taking $f \in P_3$ we can write
    \[
        x \frac{d}{dx} = x (\alpha_0  + \alpha_1 x + \alpha_2 x^2 + \alpha_3 x^3) = \alpha_1 x + 2 \alpha_2 x^2 + 3 \alpha_3 x^3
    \]
    and also
    \[
    x^2 \frac{d^2}{dx^2} f =   2 \alpha_2 x^2 + 6\alpha_3 x^3
    \]
    so we see both of those operators are linear as well. Hence since linear combinations of linear transformation are linear, $\mathcal{L}$ must be linear.
    \item First, since $f_0$ is constant and $\mathcal{L}$ acts by differentiation at least once,
    \[
    \mathcal{L} f_0 = 0,
    \]
    so $f_0$ is an eigenfunction with eigenvalue 0. Next,
    \begin{align*}
        \mathcal{L} f_1 &= (1-x^2) \frac{d^2}{dx^2}\left(\sqrt{\frac{3}{2}}x\right) - 2x \frac{d}{dx} \left( \sqrt{\frac{3}{2}}x \right) \\
    &= 2 \sqrt{\frac{3}{2}} x
    \end{align*}
    So $f_1$ is an eigenfunction with eigenvalue $1(1+1)=2$. Next,
    \begin{align*}
        \mathcal{L} f_2 &= (1-x^2) \frac{d^2}{dx^2}\left(\sqrt{\frac{5}{8}} (1-3x^2)\right) - 2x \frac{d}{dx} \left( \sqrt{\frac{5}{8}} (1-3x^2) \right) \\
    &= 6 \left( \sqrt{\frac{5}{8}} (1-3x^2) \right)
    \end{align*}
   So $f_2$ is an eigenfunction with eigenvalue $2(2+1)=6$. Finally,
    \begin{align*}
        \mathcal{L} f_3 &= (1-x^2) \frac{d^2}{dx^2}\left(\sqrt{\frac{63}{8}}\left(x-\frac{5x^3}{3}\right)\right) - 2x \frac{d}{dx} \left( \sqrt{\frac{63}{8}}\left(x-\frac{5x^3}{3}\right)\right) \\
    &= 12 \left( \sqrt{\frac{63}{8}}\left(x-\frac{5x^3}{3}\right) \right)
    \end{align*}
    So $f_3$ is an eigenfunction with eigenvalu $3(3+1)=12$. So the subscript $j$ for $f_j$ corresponds to the $\alpha$ in Homework 6 Problem 4 or the $m$ in the text.
\end{enumerate}
\end{solution}






%\newpage
%\begin{problem}~
%\begin{enumerate}[(a)]
%    \item What does a zero determinant indicate about the solutions of a non-homogeneous system of linear equations? (Think geometrically!)
%    \item What does a zero determinant indicate about the solutions of a homogeneous system of linear equatoins? (Think geometrically!)
%\end{enumerate}
%\end{problem}
%\begin{solution}~
%\begin{enumerate}[(a)]
%
%    \item If the determinant of a matrix is zero, that means the span of the columns is not the whole entire vector space.  Take for example the matrix,
%\[
%[A]=\begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix}.
%\]
%More explanation for the transformation given by this matrix is found in Chapter 9 \S 6 of the text. I'll copy the visualization over.
%        \begin{figure}[H]
%            \centering
%            \resizebox{.6\textwidth}{!}{\input{linear_transform_kernel.pdf_tex}}
%            %\includegraphics[width=.7\textwidth]{Figures_Part_4/linear_transform_kernel.pdf_tex}
%        \end{figure}
%Notice that this transformation finds the ``shadow" of the vector in the $xy$-plane. One can note now that columns of this matrix only span the $xy$-plane and this is realized geometrically as for any vector $\vecv$, $[A]\vecv$ will only have a zero $z$-component. For example,
%\[
%\begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix} \begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} x \\ y \\ 0 \end{pmatrix}.
%\]
%Hence, for sake of example, if I take the inhomogeneous equation
%\[
%\begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix} \begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}
%\]
%we get the equation
%\[
%x\xhat + y\yhat = \zhat,
%\]
%which has no solution! Geometrically, this is due to the fact that the span of the columns of $[A]$ doesn't contain the vector $\zhat$. Hence, a zero determinant means we cannot necessarily solve non-homogeneous equations.  If we instead had the equation
%\[
%\begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix} \begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix},
%\]
%then this gives us the equation
%\[
%x\xhat + y\yhat = \xhat + \yhat,
%\]
%which means we must have $x=1$ and $y=1$. However, $z$ is free to be anything, which means any vector of the form
%\[
%\begin{pmatrix} 1 \\ 1 \\ z\end{pmatrix},
%\]
%is a solution! But, this is not a unique solution.
%
%    \item A zero determinant for a matrix $[A]$ is equivalent to the fact that the columns are linearly dependent. Thus, this means some of the vectors are redundant. If $[A]$ is an $n\times n$-matrix, then the columns of $[A]$ could span at most $n-1$ dimensions and there must be at least one direction that is transformed to zero under the matrix $[A]$.
%
%    Take my example from the previous problem.  There the columns only spanned the $xy$-plane and thus the $z$-direction is squished to zero by that matrix.  So there were infinitely many solutions to the homogeneous equation. More explicitly, take a vector $\vecv = z\zhat$, then
%    \[
%    [A]\vecv = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix} \begin{pmatrix} 0 \\ 0 \\ z \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}.
%    \]
%    Geometrically, this is saying that the nullspace of $[A]$ is spanned by the vector $\zhat$ and so any scalar copy of $\zhat$ is also in the nullspace. One can picture this as having the transformation take the whole $z$-axis and squish it down to a length of zero.
%\end{enumerate}
%\end{solution}




\end{document}