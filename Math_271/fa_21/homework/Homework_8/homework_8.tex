%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document data
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt]{article} %report allows for chapters
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{preamble}
\usepackage{amssymb}

\newcommand{\vhat}{\boldsymbol{\hat{v}}}
\newcommand{\ehat}{\boldsymbol{\hat{e}}}
\newcommand{\Span}{\operatorname{Span}}

\begin{document}

\begin{center}
   \textsc{\large MATH 271, Homework 8}\\
   \textsc{Due November 16$^\textrm{th}$}
\end{center}
\vspace{.5cm}


\begin{problem}
Consider the following vectors in space $\R^3$
\[
\vecu = \ehat_1 + 2\ehat_2 + 3\ehat_3 \qquad \textrm{and} \qquad \vecv = -2\ehat_1 +\ehat_2 -2\ehat_3.
\]
\begin{enumerate}[(a)]
    \item Compute the dot product $\vecu\cdot \vecv$.
    \item Compute the lengths $|\vecu|$ and $|\vecv|$ using the dot product.
    \item Compute the projection of $\vecu$ in the direction of $\vecv$.
    \emph{Hint: don't forget to normalize the vectors before you build your projection.}
    \item Compute the cross product $\vecu \times \vecv$.
    \item Find the area of the parallelogram generated by $\vecu$ and $\vecv$.
\end{enumerate}
\end{problem}
\vspace*{0.5cm}

\begin{problem}
Write down the matrix for the following linear transformation $T\colon \R^3 \to \R^3$:
\[
T\begin{pmatrix} x\\ y\\ z \end{pmatrix}
= \begin{pmatrix} x+y+z\\ 2x\\ 3y + z \end{pmatrix}.
\]
\end{problem}

\begin{problem}
Consider the linear transformation $J \colon R^2 \to \R^2$ defined by
\[
J(\ehat_1) = \ehat_2 \qquad \textrm{and} \qquad J(\ehat_2) = -\ehat_1.
\]
This linear transformation is fundamental in understanding how we can reconstruct complex numbers using matrices.
\begin{enumerate}[(a)]
    \item Show that $J^2 = J\circ J= -1$.
    \item Determine a matrix representation for $J$ and denote it by $[J]$.
    \item Recall that we can represent a complex number as $z=x + iy$ and that we can represent $z$ as a vector in $\R^2$ as $\vec{\boldsymbol{\zeta}} = x\ehat_1 + y\ehat_2$.  Show that $J \vec{\boldsymbol{\zeta}}$ corresponds to $iz$. \emph{Hint: just show the multiplcations are analogous.}
    \item We can completely reconstruct a representation of $\C$ by using a matrix representation.  In particular, we can take
    \[
        [z] = x [I] + y [J].
    \]
    Show that we recover the complex addition and multiplication using this representation.
    \item We can represent a unit complex number as $z=e^{i\theta}$.  Show that the representation described before leads to
    \[
        [z] = \begin{pmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{pmatrix}.
    \]
\end{enumerate}
%\hrule
%\vspace*{5pt}
%You've now concretely realized that complex numbers can be fully represented using linear algebra and arise as nothing more than some type of special matrices.  In particular, you've represented the $\C$ algebra in (d) and the unitary group $\operatorname{U}(1)$ in (e).
%
%To throw some more bonus technical terms out there, you've also realized the Lie algebra associated to the Lie group $\operatorname{U}(1)$, $\mathfrak{u}(1)$ is none other than $\R$ (the $\theta$ that shows up in $e^{i\theta}$) and that $\operatorname{U}(1)$ just happens to be a nice way to think about $\operatorname{Spin}(2)$.  Remember, we can rotate any complex number $z$ by letting $e^{i \theta}$ multiply $z$ which is a convenient way of seeing (what is essentially) the adjoint representation of $\operatorname{Spin}(2)$ ($e^{i\theta}$ in this case) on $\mathfrak{spin}(2)$ ($z$ in this case) which is equivalent to $\C$. I don't expect you to understand these terms, but when you do, you'd be well on your way to Dirac's theory of the electron!
%
%We will discuss groups and symmetries later on in 271 and the group $\operatorname{U}(1)$ will show up in yet another guise as rotational symmetries of the plane which we write as $\operatorname{SO}(2)$ which just so happens to be the same as $\operatorname{U}(1)$.  What's the point? It just goes to show that there are so many ways of thinking about these structures we have used and they provide convenient tools to break down important physical problems. For example, in 272 we will find the symmetries under $\operatorname{U}(1)$ are important for understanding the Sch\"odinger equation and the Fourier transform.
%\vspace*{5pt}
%\hrule
\end{problem}

\begin{problem}
    Take the following matrices:
    \[
        [A] = \begin{pmatrix} 4 & 3 & 10 & 2 \\ 1 & 1 & 0 & 9 \end{pmatrix},\quad [B] = \begin{pmatrix} 8 & 5 & 8 \\ 10 & 9 & 2 \\ 4 & 6 &3 \end{pmatrix}, \quad [C] = \begin{pmatrix} 0 & 0 & 9 \\ 7 & 9 & 9 \\ 1 & 9 & 9 \\ 3 & 3 & 1\end{pmatrix}
    \]
    \begin{enumerate}[(a)]
        \item Compute either $[A][C]$ or $[C][A]$ and state which multiplication is not possible.
        \item Compute either $[B][C]$ or $[C][B]$ and state which multiplication is not possible.
        \item Can you add any of these matrices?
        \item Describe each matrix as linear transformation $T\colon \R^m \to \R^n$. What is $m$ and $n$ for each? How does this relate to the number of rows and columns?
    \end{enumerate}
\end{problem}


\begin{problem}
Solve the following equation.
\[
\begin{pmatrix} 1 & 1 & 1 \\ 1 & 2 & 1 \\ 1 & 2 & 2 \end{pmatrix} \begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} 6 \\ 8 \\ 11 \end{pmatrix}.
\]
\end{problem}

\begin{problem}
Consider the space of polynomials of degree at most 3, $P_3(\C)$.
\begin{enumerate}[(a)]
    \item Using the basis
    \[
        B = \{ 1, x, x^2, x^3\},
    \]
    determine a matrix representation for the linear transformation $\frac{d}{dx} \colon P_3(\C) \to P_3(\C)$.
    \item Show that the set of Legendre polynomials
    \[
        B_L = \left\{f_0 = \sqrt{\frac{1}{2}}, ~ f_1 = \sqrt{\frac{3}{2}}x, ~ f_2 = \sqrt{\frac{5}{8}} (1-3x^2),~ f_3=\sqrt{\frac{63}{8}}\left(x-\frac{5x^3}{3}\right) \right\}
    \]
    is a basis for $P_3(\C)$.
    \item Using the basis $B_L$ instead, compute a matrix representation for the linear transformation $\frac{d}{dx}$.
\end{enumerate}
\begin{remark}
    This should go to show you that a matrix representation depends on a basis!!!
\end{remark}
\end{problem}

\begin{problem}
Let $C^\omega(\C)$ be the set of analytic functions (functions that have a power series representation), i.e., functions of the form
\[
f(x) = \sum_{n=0}^\infty a_n x^n,
\]
where $a_n\in \C$ for $n=0,1,2,\dots$. Let us compare this with the vector space of polynomials $P_N(\C)$.
\begin{enumerate}[(a)]
    \item Argue that $C^\omega(\C)$ is a vector space. \emph{Hint: show what addition and scalar multiplication look like.}
    \item Show that the space of polynomials of degree at most $N$, $P_N(\C)$ is a subspace of $C^\omega(\C)$.
    \item Let $f,g\in C^\omega(\C)$ be given by
    \[
        f(x)=\sum_{n=0}^\infty a_n x^n \qquad \textrm{and} \qquad g(x) = \sum_{n=0}^\infty b_n x^n,
    \]
    then define an inner product on $C^\omega(\C)$ by taking
    \[
    \langle f,g\rangle \coloneqq \sum_{n=0}^\infty a_n b_n^*.
    \]
    Now, write $h(x)$ as a Taylor series centered at $x=0$ and show that
    \[
    h^{(n)}(0) = n!\langle h,x^n \rangle.
    \]
    \item Show that the $N^\textrm{th}$ order Taylor approximation for the function $h(x)$ centered at $x=0$ is the projection onto the subspace spanned by the functions
    \[
    S=\{1,x,x^2,\dots,x^N\}.
    \]
    This projection is given by
    \[
    \textrm{proj}_S(h) = \sum_{n=0}^N \langle h, x^n \rangle x^n.
    \]
\end{enumerate}
\end{problem}






\end{document}